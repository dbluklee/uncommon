"""
ë²¡í„° ê²€ìƒ‰ ëª¨ë“ˆ
Milvusë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê²€ìƒ‰
"""

import os
import logging
from typing import List, Dict, Any, Optional
from pymilvus import Collection, connections, utility
import json
import time
import asyncio
from datetime import datetime
import asyncpg

logger = logging.getLogger(__name__)

class VectorSearcher:
    """Milvus ë²¡í„° ê²€ìƒ‰ê¸°"""
    
    def __init__(self, embedding_generator):
        """ë²¡í„° ê²€ìƒ‰ê¸° ì´ˆê¸°í™”"""
        self.embedding_generator = embedding_generator
        self.collection_name = os.environ["COLLECTION_NAME"]
        
        # PostgreSQL ì—°ê²° ì •ë³´
        self.postgres_config = {
            "host": os.environ["POSTGRES_HOST"],
            "port": int(os.environ["POSTGRES_PORT"]),
            "database": os.environ["POSTGRES_DB"],
            "user": os.environ["POSTGRES_USER"],
            "password": os.environ["POSTGRES_PASSWORD"]
        }
        
        # Milvus ì—°ê²°
        self._connect_milvus()
        
        # ì»¬ë ‰ì…˜ ë¡œë“œ
        self._load_collection()
    
    def _connect_milvus(self):
        """Milvus ì„œë²„ ì—°ê²°"""
        try:
            milvus_host = os.environ["MILVUS_HOST"]
            milvus_port = os.environ["MILVUS_INTERNAL_PORT"]
            
            logger.info(f"ğŸ”— Milvus ì—°ê²° ì‹œë„: {milvus_host}:{milvus_port}")
            
            connections.connect(
                alias="default",
                host=milvus_host,
                port=milvus_port,
                timeout=30
            )
            
            # ì—°ê²° í™•ì¸
            if utility.has_collection(self.collection_name):
                logger.info(f"âœ… Milvus ì—°ê²° ì„±ê³µ! ì»¬ë ‰ì…˜: {self.collection_name}")
            else:
                logger.warning(f"âš ï¸ ì»¬ë ‰ì…˜ '{self.collection_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                
        except Exception as e:
            logger.error(f"âŒ Milvus ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _load_collection(self):
        """ì»¬ë ‰ì…˜ ë¡œë“œ"""
        try:
            if not utility.has_collection(self.collection_name):
                logger.error(f"ì»¬ë ‰ì…˜ '{self.collection_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                return
            
            self.collection = Collection(name=self.collection_name)
            self.collection.load()
            
            # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
            num_entities = self.collection.num_entities
            logger.info(f"ğŸ“Š ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ: {num_entities}ê°œ ë²¡í„°")
            
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            embed_start = time.time()
            logger.info(f"ğŸ” ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±: {query[:50]}...")
            query_embedding = self.embedding_generator.generate_query_embedding(query)
            embed_end = time.time()
            logger.info(f"â±ï¸ ì„ë² ë”© ìƒì„± ì‹œê°„: {embed_end - embed_start:.3f}ì´ˆ")
            
            # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° (í™˜ê²½ë³€ìˆ˜ì˜ METRIC_TYPE ì‚¬ìš©)
            metric_type = os.environ.get("METRIC_TYPE", "COSINE")
            search_params = {
                "metric_type": metric_type,
                "params": {"nprobe": 10}
            }
            
            # ì¶œë ¥ í•„ë“œ ì„¤ì •
            output_fields = ["product_id", "product_name", "chunk_type", "source", "content"]
            
            # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            milvus_start = time.time()
            logger.info(f"ğŸ” Milvus ê²€ìƒ‰ ì¤‘ (top_k={top_k})...")
            search_results = self.collection.search(
                data=[query_embedding],
                anns_field="vector",
                param=search_params,
                limit=top_k,
                output_fields=output_fields
            )
            milvus_end = time.time()
            logger.info(f"â±ï¸ Milvus ê²€ìƒ‰ ì‹œê°„: {milvus_end - milvus_start:.3f}ì´ˆ")
            
            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_results = []
            for hits in search_results:
                for hit in hits:
                    # Milvus ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•„ë“œ ì¶”ì¶œ (hit.fields ì‚¬ìš©)
                    fields = hit.fields
                    result = {
                        "id": hit.id,
                        "score": float(hit.score),
                        "content": fields.get("content", ""),
                        "product_id": fields.get("product_id", 0),
                        "product_name": fields.get("product_name", ""),
                        "chunk_type": fields.get("chunk_type", ""),
                        "source": fields.get("source", "")
                    }
                    formatted_results.append(result)
            
            logger.info(f"âœ… {len(formatted_results)}ê°œ ê²°ê³¼ ê²€ìƒ‰ ì™„ë£Œ")
            
            # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ (ë†’ì€ ì ìˆ˜ ìš°ì„ )
            formatted_results.sort(key=lambda x: x["score"], reverse=True)
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def search_with_filter(self, query: str, filter_expr: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        í•„í„°ë¥¼ í¬í•¨í•œ ë²¡í„° ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            filter_expr: Milvus í•„í„° í‘œí˜„ì‹ (ì˜ˆ: "product_id in [1, 2, 3]")
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embedding_generator.generate_query_embedding(query)
            
            # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
            search_params = {
                "metric_type": "IP",
                "params": {"nprobe": 10}
            }
            
            # í•„í„°ë¥¼ í¬í•¨í•œ ê²€ìƒ‰
            search_results = self.collection.search(
                data=[query_embedding],
                anns_field="vector",
                param=search_params,
                limit=top_k,
                expr=filter_expr,
                output_fields=["product_id", "product_name", "chunk_type", "source", "content"]
            )
            
            # ê²°ê³¼ í¬ë§·íŒ… (ìœ„ì™€ ë™ì¼)
            formatted_results = []
            for hits in search_results:
                for hit in hits:
                    # Milvus ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•„ë“œ ì¶”ì¶œ (hit.fields ì‚¬ìš©)
                    fields = hit.fields
                    result = {
                        "id": hit.id,
                        "score": float(hit.score),
                        "content": fields.get("content", ""),
                        "product_id": fields.get("product_id", 0),
                        "product_name": fields.get("product_name", ""),
                        "chunk_type": fields.get("chunk_type", ""),
                        "source": fields.get("source", "")
                    }
                    formatted_results.append(result)
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"í•„í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def get_collection_stats(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            if not hasattr(self, 'collection'):
                return {"error": "Collection not loaded"}
            
            return {
                "collection_name": self.collection_name,
                "row_count": self.collection.num_entities,
                "loaded": True
            }
            
        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}
    
    def release_collection(self):
        """ì»¬ë ‰ì…˜ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ í•´ì œ)"""
        try:
            if hasattr(self, 'collection'):
                self.collection.release()
                logger.info(f"ì»¬ë ‰ì…˜ '{self.collection_name}' ì–¸ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ì–¸ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    # ========== ê´€ë¦¬ì ê¸°ëŠ¥ ë©”ì„œë“œë“¤ ==========
    
    async def _get_postgres_connection(self):
        """PostgreSQL ì—°ê²° ìƒì„±"""
        try:
            return await asyncpg.connect(**self.postgres_config)
        except Exception as e:
            logger.error(f"PostgreSQL ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            raise

    async def get_document_stats(self) -> Dict[str, Any]:
        """PostgreSQLì˜ ë¬¸ì„œ í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            conn = await self._get_postgres_connection()
            
            # ì´ ë¬¸ì„œ ìˆ˜
            total_docs = await conn.fetchval("SELECT COUNT(*) FROM documents")
            
            # ì¸ë±ì‹±ëœ ë¬¸ì„œ ìˆ˜
            indexed_docs = await conn.fetchval("SELECT COUNT(*) FROM documents WHERE indexed = true")
            
            # ëŒ€ê¸° ì¤‘ì¸ ë¬¸ì„œ ìˆ˜
            pending_docs = total_docs - indexed_docs
            
            # ìµœê·¼ ì—…ë°ì´íŠ¸
            last_update = await conn.fetchval(
                "SELECT MAX(indexed_at) FROM documents WHERE indexed_at IS NOT NULL"
            )
            
            await conn.close()
            
            return {
                "total_documents": total_docs,
                "indexed_documents": indexed_docs,
                "pending_documents": pending_docs,
                "last_update": str(last_update) if last_update else None
            }
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}

    async def get_all_documents(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
        try:
            conn = await self._get_postgres_connection()
            
            # documents í…Œì´ë¸”ì—ì„œ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ
            query = """
                SELECT 
                    id, title, url, content,
                    indexed, scraped_at, indexed_at
                FROM documents 
                ORDER BY scraped_at DESC
            """
            
            rows = await conn.fetch(query)
            
            documents = []
            for row in rows:
                doc = {
                    "id": row["id"],
                    "title": row["title"] or row["url"] or f"ë¬¸ì„œ {row['id']}",
                    "content": row["content"][:200] + "..." if len(row["content"]) > 200 else row["content"],
                    "category": "scraped" if row["url"] else "manual",
                    "indexed": row["indexed"],
                    "created_at": row["scraped_at"] or datetime.utcnow(),
                    "indexed_at": row["indexed_at"]
                }
                documents.append(doc)
            
            await conn.close()
            return documents
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []

    async def get_document_vector_count(self, doc_id: int) -> int:
        """íŠ¹ì • ë¬¸ì„œì˜ ë²¡í„° ê°œìˆ˜ ì¡°íšŒ"""
        try:
            # Milvusì—ì„œ í•´ë‹¹ ë¬¸ì„œì˜ ë²¡í„° ê°œìˆ˜ ì¡°íšŒ
            expr = f"product_id == {doc_id}"
            query_result = self.collection.query(expr=expr, output_fields=["id"])
            return len(query_result)
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê°œìˆ˜ ì¡°íšŒ ì‹¤íŒ¨ (doc_id: {doc_id}): {str(e)}")
            return 0

    async def add_manual_document(self, title: str, content: str, category: str = "manual") -> int:
        """ìˆ˜ë™ìœ¼ë¡œ ë¬¸ì„œ ì¶”ê°€"""
        try:
            conn = await self._get_postgres_connection()
            
            # documents í…Œì´ë¸”ì— ì‚½ì… (urlì€ NULL, title ì‚¬ìš©)
            query = """
                INSERT INTO documents (title, content, indexed, scraped_at)
                VALUES ($1, $2, false, NOW())
                RETURNING id
            """
            
            doc_id = await conn.fetchval(query, title, content)
            
            await conn.close()
            logger.info(f"âœ… ìˆ˜ë™ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ: ID={doc_id}, ì œëª©={title}")
            
            return doc_id
            
        except Exception as e:
            logger.error(f"ìˆ˜ë™ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
            raise

    async def index_document(self, doc_id: int) -> bool:
        """íŠ¹ì • ë¬¸ì„œë¥¼ ë²¡í„°í™”í•˜ì—¬ Milvusì— ì €ì¥"""
        try:
            conn = await self._get_postgres_connection()
            
            # ë¬¸ì„œ ì •ë³´ ì¡°íšŒ
            query = "SELECT title, content FROM documents WHERE id = $1 AND indexed = false"
            doc = await conn.fetchrow(query, doc_id)
            
            if not doc:
                logger.warning(f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì´ë¯¸ ì¸ë±ì‹±ë¨: {doc_id}")
                await conn.close()
                return False
            
            # ë¬¸ì„œ ì²­í‚¹ (ê°„ë‹¨í•œ ì˜ˆì œ - ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì²­í‚¹ í•„ìš”)
            title = doc["title"] or ""
            content = doc["content"]
            
            # ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì²˜ë¦¬
            chunk_text = f"{title}\n\n{content}"
            
            # ì„ë² ë”© ìƒì„±
            embedding = self.embedding_generator.generate_embedding(chunk_text)
            
            # Milvusì— ì €ì¥
            data = [{
                "id": doc_id * 1000,  # ê³ ìœ  ID (ë¬¸ì„œ ID * 1000)
                "vector": embedding,
                "product_id": doc_id,
                "product_name": title,
                "chunk_type": "manual_document",
                "content": chunk_text[:1000],  # ìµœëŒ€ 1000ì
                "source": f"manual_doc_{doc_id}"
            }]
            
            self.collection.insert(data)
            self.collection.flush()
            
            # PostgreSQLì—ì„œ ì¸ë±ì‹± ì™„ë£Œë¡œ í‘œì‹œ
            update_query = "UPDATE documents SET indexed = true, indexed_at = NOW() WHERE id = $1"
            await conn.execute(update_query, doc_id)
            
            await conn.close()
            logger.info(f"âœ… ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ: ID={doc_id}")
            
            return True
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì¸ë±ì‹± ì‹¤íŒ¨ (doc_id: {doc_id}): {str(e)}")
            return False

    async def delete_document_vectors(self, doc_id: int) -> bool:
        """Milvusì—ì„œ íŠ¹ì • ë¬¸ì„œì˜ ëª¨ë“  ë²¡í„° ì‚­ì œ"""
        try:
            # í•´ë‹¹ ë¬¸ì„œì˜ ëª¨ë“  ë²¡í„° ì‚­ì œ
            expr = f"product_id == {doc_id}"
            self.collection.delete(expr)
            self.collection.flush()
            
            logger.info(f"âœ… Milvus ë²¡í„° ì‚­ì œ ì™„ë£Œ: doc_id={doc_id}")
            return True
            
        except Exception as e:
            logger.error(f"Milvus ë²¡í„° ì‚­ì œ ì‹¤íŒ¨ (doc_id: {doc_id}): {str(e)}")
            return False

    async def delete_document(self, doc_id: int) -> bool:
        """PostgreSQLì—ì„œ ë¬¸ì„œ ì‚­ì œ"""
        try:
            conn = await self._get_postgres_connection()
            
            # ë¬¸ì„œ ì‚­ì œ
            query = "DELETE FROM documents WHERE id = $1"
            result = await conn.execute(query, doc_id)
            
            await conn.close()
            
            # ì‚­ì œëœ í–‰ ìˆ˜ í™•ì¸
            if result == "DELETE 1":
                logger.info(f"âœ… PostgreSQL ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: ID={doc_id}")
                return True
            else:
                logger.warning(f"âš ï¸ ì‚­ì œí•  ë¬¸ì„œ ì—†ìŒ: ID={doc_id}")
                return False
                
        except Exception as e:
            logger.error(f"PostgreSQL ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨ (doc_id: {doc_id}): {str(e)}")
            return False