"""
UNCOMMON RAG API Service
ì‚¬ìš©ì ì§ˆì˜ë¥¼ ì²˜ë¦¬í•˜ê³  ê´€ë ¨ ì œí’ˆ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
"""

import os
import logging
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException, Depends, status, File, UploadFile, Form
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
from dotenv import load_dotenv
import json
import asyncio
import time
import hashlib
from datetime import datetime, timedelta
import jwt

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from embedding_generator import EmbeddingGenerator
from vector_search import VectorSearcher
from llm_client import LLMClient

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•±
app = FastAPI(
    title="UNCOMMON RAG API Service",
    description="ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ì œí’ˆ ì •ë³´ ì§ˆì˜ì‘ë‹µ ì„œë¹„ìŠ¤",
    version="1.0.0"
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
embedding_generator = None
vector_searcher = None
llm_client = None

# ê´€ë¦¬ì ì¸ì¦ ì„¤ì •
security = HTTPBearer()
ADMIN_USERNAME = os.getenv("ADMIN_USERNAME", "admin")
ADMIN_PASSWORD_HASH = hashlib.sha256(os.getenv("ADMIN_PASSWORD", "admin123").encode()).hexdigest()
JWT_SECRET_KEY = os.getenv("JWT_SECRET_KEY", "uncommon_secret_key_2024")
JWT_ALGORITHM = "HS256"
JWT_EXPIRE_HOURS = 24

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë©”ëª¨ë¦¬ì— ì €ì¥, ì‹¤ì œë¡œëŠ” DBë‚˜ íŒŒì¼ì— ì €ì¥)
SYSTEM_PROMPT = """ë‹¤ìŒì€ UNCOMMON ì•ˆê²½ ì œí’ˆì— ëŒ€í•œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê´€ë ¨ ì œí’ˆ ì •ë³´:
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. 
- ì œí’ˆëª…, ê°€ê²©, íŠ¹ì§•ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
- í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”
- ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ë‹¤ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”"""

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class ChatRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5
    stream: Optional[bool] = True
    temperature: Optional[float] = 0.7
    include_images: Optional[bool] = False
    include_debug: Optional[bool] = False

class MultiModalChatRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5
    stream: Optional[bool] = True
    temperature: Optional[float] = 0.7
    include_debug: Optional[bool] = False

class ChatResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    query_embedding_dim: int

class SearchRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5

class SearchResponse(BaseModel):
    results: List[Dict[str, Any]]
    query: str
    total_results: int

# ê´€ë¦¬ì ëª¨ë¸ë“¤
class AdminLoginRequest(BaseModel):
    username: str
    password: str

class AdminLoginResponse(BaseModel):
    token: str
    user: str
    expires_at: datetime

class SystemPromptRequest(BaseModel):
    prompt: str

class SystemPromptResponse(BaseModel):
    prompt: str
    updated_at: datetime

class DocumentCreateRequest(BaseModel):
    title: str
    content: str
    category: str = "manual"

class DocumentResponse(BaseModel):
    id: int
    title: str
    content: str
    category: str
    created_at: datetime
    vector_count: int

# ê´€ë¦¬ì ì¸ì¦ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def create_jwt_token(username: str) -> str:
    """JWT í† í° ìƒì„±"""
    payload = {
        "sub": username,
        "exp": datetime.utcnow() + timedelta(hours=JWT_EXPIRE_HOURS),
        "iat": datetime.utcnow()
    }
    return jwt.encode(payload, JWT_SECRET_KEY, algorithm=JWT_ALGORITHM)

def verify_jwt_token(token: str) -> Optional[str]:
    """JWT í† í° ê²€ì¦"""
    try:
        payload = jwt.decode(token, JWT_SECRET_KEY, algorithms=[JWT_ALGORITHM])
        username = payload.get("sub")
        if username is None:
            return None
        return username
    except jwt.PyJWTError:
        return None

def verify_admin_credentials(username: str, password: str) -> bool:
    """ê´€ë¦¬ì ì¸ì¦ í™•ì¸"""
    password_hash = hashlib.sha256(password.encode()).hexdigest()
    return username == ADMIN_USERNAME and password_hash == ADMIN_PASSWORD_HASH

async def get_current_admin(credentials: HTTPAuthorizationCredentials = Depends(security)) -> str:
    """í˜„ì¬ ê´€ë¦¬ì ì‚¬ìš©ì í™•ì¸"""
    username = verify_jwt_token(credentials.credentials)
    if username is None:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ì¦ í† í°ì…ë‹ˆë‹¤",
            headers={"WWW-Authenticate": "Bearer"},
        )
    return username

@app.on_event("startup")
async def startup_event():
    """ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global embedding_generator, vector_searcher, llm_client
    
    try:
        logger.info("ğŸš€ UNCOMMON RAG API ì„œë¹„ìŠ¤ ì‹œì‘")
        
        # ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”
        logger.info("ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        embedding_generator = EmbeddingGenerator()
        logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # ë²¡í„° ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        logger.info("ğŸ”— Milvus ë²¡í„° ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        vector_searcher = VectorSearcher(embedding_generator)
        logger.info("âœ… Milvus ë²¡í„° ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        logger.info("ğŸ¤– Ollama LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        llm_client = LLMClient()
        logger.info("âœ… Ollama LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        logger.info("ğŸ‰ ëª¨ë“  ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        raise

@app.get("/")
async def root():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "service": "UNCOMMON RAG API Service",
        "status": "running",
        "version": "1.0.0",
        "embedding_model": os.getenv("EMBEDDING_MODEL", "BGE-M3"),
        "llm_model": os.getenv("OLLAMA_MODEL", "gemma3"),
        "vector_store": "Milvus"
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy"}

@app.post("/search", response_model=SearchResponse)
async def search_products(request: SearchRequest):
    """
    ì œí’ˆ ë²¡í„° ê²€ìƒ‰ (LLM ì—†ì´ ê²€ìƒ‰ë§Œ)
    """
    try:
        logger.info(f"ğŸ” ê²€ìƒ‰ ìš”ì²­: {request.query}")
        
        # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        search_results = await vector_searcher.search(
            query=request.query,
            top_k=request.top_k
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for result in search_results:
            formatted_results.append({
                "content": result.get("content", ""),
                "product_id": result.get("product_id", 0),
                "product_name": result.get("product_name", ""),
                "chunk_type": result.get("chunk_type", ""),
                "source": result.get("source", ""),
                "score": result.get("score", 0.0)
            })
        
        logger.info(f"âœ… {len(formatted_results)}ê°œ ê²°ê³¼ ë°˜í™˜")
        
        return SearchResponse(
            results=formatted_results,
            query=request.query,
            total_results=len(formatted_results)
        )
        
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat")
async def chat(request: ChatRequest):
    """
    RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
    """
    try:
        start_time = time.time()
        logger.info(f"ğŸ’¬ ì±„íŒ… ìš”ì²­: {request.query}")
        
        # 1. ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        search_start = time.time()
        search_results = await vector_searcher.search(
            query=request.query,
            top_k=request.top_k
        )
        search_end = time.time()
        logger.info(f"â±ï¸ ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {search_end - search_start:.2f}ì´ˆ")
        
        if not search_results:
            logger.warning("âš ï¸ ê´€ë ¨ ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return ChatResponse(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì œí’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                sources=[],
                query_embedding_dim=1024
            )
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_start = time.time()
        context = _build_context(search_results)
        context_end = time.time()
        logger.info(f"â±ï¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {context_end - context_start:.2f}ì´ˆ")
        
        # 3. LLM ì‘ë‹µ ìƒì„±
        if request.stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            logger.info("ğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
            return StreamingResponse(
                _stream_response(request.query, context, search_results, request.temperature, request),
                media_type="text/event-stream"
            )
        else:
            # ì¼ë°˜ ì‘ë‹µ
            llm_start = time.time()
            logger.info("ğŸ¤– LLM ì‘ë‹µ ìƒì„± ì‹œì‘...")
            answer = await llm_client.generate(
                query=request.query,
                context=context,
                temperature=request.temperature
            )
            llm_end = time.time()
            total_end = time.time()
            
            logger.info(f"â±ï¸ LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ: {llm_end - llm_start:.2f}ì´ˆ")
            logger.info(f"â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_end - start_time:.2f}ì´ˆ")
            
            response_data = ChatResponse(
                answer=answer,
                sources=_format_sources(search_results),
                query_embedding_dim=1024
            )
            
            # ë””ë²„ê¹… ì •ë³´ê°€ ìš”ì²­ëœ ê²½ìš° ì¶”ê°€
            if request.include_debug:
                debug_info = _build_debug_info(request.query, search_results, context, request)
                # ChatResponse ëª¨ë¸ì— debug_info í•„ë“œë¥¼ ì¶”ê°€í•˜ê±°ë‚˜, dictë¡œ ë°˜í™˜
                return {
                    "answer": answer,
                    "sources": _format_sources(search_results),
                    "query_embedding_dim": 1024,
                    "debug_info": debug_info
                }
            
            return response_data
            
    except Exception as e:
        logger.error(f"âŒ ì±„íŒ… ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat/multimodal")
async def multimodal_chat(
    query: str = Form(...),
    top_k: Optional[int] = Form(5),
    stream: Optional[bool] = Form(True), 
    temperature: Optional[float] = Form(0.7),
    include_debug: Optional[bool] = Form(False),
    image: Optional[UploadFile] = File(None)
):
    """
    ë©€í‹°ëª¨ë‹¬ RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸)
    """
    try:
        start_time = time.time()
        logger.info(f"ğŸ–¼ï¸ ë©€í‹°ëª¨ë‹¬ ì±„íŒ… ìš”ì²­: {query}")
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬
        image_data = None
        if image and image.size > 0:
            # ì´ë¯¸ì§€ í¬ê¸° ì œí•œ (10MB)
            if image.size > 10 * 1024 * 1024:
                raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ í¬ê¸°ê°€ 10MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤")
            
            # ì§€ì›ë˜ëŠ” ì´ë¯¸ì§€ í˜•ì‹ í™•ì¸
            allowed_types = ['image/jpeg', 'image/jpg', 'image/png', 'image/gif', 'image/webp']
            if image.content_type not in allowed_types:
                raise HTTPException(status_code=400, detail="ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤. (JPEG, PNG, GIF, WebP ì§€ì›)")
            
            image_data = await image.read()
            logger.info(f"ğŸ“· ì´ë¯¸ì§€ ì—…ë¡œë“œë¨: {image.filename}, í¬ê¸°: {len(image_data)} bytes, íƒ€ì…: {image.content_type}")
        
        # 1. ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        search_start = time.time()
        search_results = await vector_searcher.search(
            query=query,
            top_k=top_k
        )
        search_end = time.time()
        logger.info(f"â±ï¸ ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {search_end - search_start:.2f}ì´ˆ")
        
        if not search_results:
            logger.warning("âš ï¸ ê´€ë ¨ ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return ChatResponse(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì œí’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                sources=[],
                query_embedding_dim=1024
            )
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_start = time.time()
        context = _build_context(search_results)
        context_end = time.time()
        logger.info(f"â±ï¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {context_end - context_start:.2f}ì´ˆ")
        
        # 3. LLM ì‘ë‹µ ìƒì„± (ì´ë¯¸ì§€ í¬í•¨)
        if stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            logger.info(f"ğŸ“¡ {'ë©€í‹°ëª¨ë‹¬ ' if image_data else ''}ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
            return StreamingResponse(
                _stream_multimodal_response(query, context, search_results, temperature, image_data, include_debug),
                media_type="text/event-stream"
            )
        else:
            # ì¼ë°˜ ì‘ë‹µ
            llm_start = time.time()
            logger.info(f"ğŸ¤– {'ë©€í‹°ëª¨ë‹¬ ' if image_data else ''}LLM ì‘ë‹µ ìƒì„± ì‹œì‘...")
            answer = await llm_client.generate(
                query=query,
                context=context,
                temperature=temperature,
                image_data=image_data
            )
            llm_end = time.time()
            total_end = time.time()
            
            logger.info(f"â±ï¸ LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ: {llm_end - llm_start:.2f}ì´ˆ")
            logger.info(f"â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_end - start_time:.2f}ì´ˆ")
            
            response_data = ChatResponse(
                answer=answer,
                sources=_format_sources(search_results),
                query_embedding_dim=1024
            )
            
            # ë””ë²„ê¹… ì •ë³´ê°€ ìš”ì²­ëœ ê²½ìš° ì¶”ê°€
            if include_debug:
                debug_info = _build_debug_info(query, search_results, context, ChatRequest(
                    query=query, top_k=top_k, temperature=temperature, stream=stream, include_debug=include_debug
                ))
                return {
                    "answer": answer,
                    "sources": _format_sources(search_results),
                    "query_embedding_dim": 1024,
                    "debug_info": debug_info,
                    "has_image": image_data is not None
                }
            
            return response_data
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë©€í‹°ëª¨ë‹¬ ì±„íŒ… ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

def _build_context(search_results: List[Dict]) -> str:
    """ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
    context_parts = []
    max_length = int(os.getenv("MAX_CONTEXT_LENGTH", 4000))
    current_length = 0
    
    for i, result in enumerate(search_results, 1):
        content = result.get("content", "")
        product_name = result.get("product_name", "")
        chunk_type = result.get("chunk_type", "")
        
        # ì œí’ˆ ì •ë³´ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context_item = f"[ì œí’ˆì •ë³´ {i}]\n"
        if product_name:
            context_item += f"ì œí’ˆëª…: {product_name}\n"
        if chunk_type:
            context_item += f"ì •ë³´ ìœ í˜•: {chunk_type}\n"
        context_item += f"{content}\n"
        
        # ê¸¸ì´ ì²´í¬
        if current_length + len(context_item) > max_length:
            break
            
        context_parts.append(context_item)
        current_length += len(context_item)
    
    return "\n".join(context_parts)

def _format_sources(search_results: List[Dict]) -> List[Dict]:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì†ŒìŠ¤ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    sources = []
    for result in search_results:
        sources.append({
            "product_name": result.get("product_name", "Unknown"),
            "product_id": result.get("product_id"),
            "chunk_type": result.get("chunk_type"),
            "score": result.get("score", 0.0)
        })
    return sources

def _build_debug_info(query: str, search_results: List[Dict], context: str, request: ChatRequest) -> Dict[str, Any]:
    """ë””ë²„ê¹… ì •ë³´ ìƒì„±"""
    return {
        "query": query,
        "search_results": [
            {
                "product_name": result.get("product_name", "Unknown"),
                "chunk_type": result.get("chunk_type", "unknown"),
                "content": result.get("content", "")[:500],  # ì²˜ìŒ 500ìë§Œ
                "score": result.get("score", 0.0),
                "product_id": result.get("product_id"),
                "source": result.get("source", "")
            }
            for result in search_results
        ],
        "prompt": SYSTEM_PROMPT.format(query=query, context=context),
        "settings": {
            "top_k": request.top_k,
            "temperature": request.temperature,
            "embedding_model": os.getenv("EMBEDDING_MODEL", "BGE-M3"),
            "llm_model": os.getenv("OLLAMA_MODEL", "gemma3"),
            "stream": request.stream,
            "max_context_length": int(os.getenv("MAX_CONTEXT_LENGTH", 4000))
        }
    }

async def _stream_response(query: str, context: str, search_results: List[Dict], temperature: float, request: ChatRequest = None):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
    try:
        # ë””ë²„ê¹… ì •ë³´ ì¤€ë¹„
        debug_info = None
        if request and request.include_debug:
            debug_info = _build_debug_info(query, search_results, context, request)
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì´ë²¤íŠ¸ (ë””ë²„ê¹… ì •ë³´ í¬í•¨)
        start_data = {
            'type': 'start', 
            'sources': _format_sources(search_results)
        }
        if debug_info:
            start_data['debug_info'] = debug_info
        
        yield f"data: {json.dumps(start_data)}\n\n"
        
        # LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        async for chunk in llm_client.stream_generate(query, context, temperature):
            yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
            await asyncio.sleep(0.01)  # ë°±í”„ë ˆì…” ì œì–´
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì´ë²¤íŠ¸
        yield f"data: {json.dumps({'type': 'end'})}\n\n"
        
    except Exception as e:
        logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}")
        yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

async def _stream_multimodal_response(query: str, context: str, search_results: List[Dict], temperature: float, image_data: Optional[bytes] = None, include_debug: bool = False):
    """ë©€í‹°ëª¨ë‹¬ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
    try:
        # ë””ë²„ê¹… ì •ë³´ ì¤€ë¹„
        debug_info = None
        if include_debug:
            debug_info = _build_debug_info(query, search_results, context, ChatRequest(
                query=query, temperature=temperature, stream=True, include_debug=include_debug
            ))
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì´ë²¤íŠ¸ (ë””ë²„ê¹… ì •ë³´ í¬í•¨)
        start_data = {
            'type': 'start', 
            'sources': _format_sources(search_results),
            'has_image': image_data is not None
        }
        if debug_info:
            start_data['debug_info'] = debug_info
        
        yield f"data: {json.dumps(start_data)}\n\n"
        
        # LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ì´ë¯¸ì§€ í¬í•¨)
        async for chunk in llm_client.stream_generate(query, context, temperature, image_data):
            yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
            await asyncio.sleep(0.01)  # ë°±í”„ë ˆì…” ì œì–´
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì´ë²¤íŠ¸
        yield f"data: {json.dumps({'type': 'end'})}\n\n"
        
    except Exception as e:
        logger.error(f"ë©€í‹°ëª¨ë‹¬ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}")
        yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

@app.get("/stats")
async def get_stats():
    """ì‹œìŠ¤í…œ í†µê³„ ì •ë³´"""
    try:
        stats = await vector_searcher.get_collection_stats()
        return {
            "collection_name": os.getenv("COLLECTION_NAME", "uncommon_products"),
            "total_vectors": stats.get("row_count", 0),
            "embedding_dim": int(os.getenv("DIMENSION", 1024)),
            "metric_type": os.getenv("METRIC_TYPE", "COSINE")
        }
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return {"error": str(e)}

# ========== ê´€ë¦¬ì API ì—”ë“œí¬ì¸íŠ¸ë“¤ ==========

@app.post("/admin/login", response_model=AdminLoginResponse)
async def admin_login(request: AdminLoginRequest):
    """ê´€ë¦¬ì ë¡œê·¸ì¸"""
    try:
        logger.info(f"ğŸ‘¤ ê´€ë¦¬ì ë¡œê·¸ì¸ ì‹œë„: {request.username}")
        
        if not verify_admin_credentials(request.username, request.password):
            logger.warning(f"âŒ ì¸ì¦ ì‹¤íŒ¨: {request.username}")
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="ì‚¬ìš©ìëª… ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤"
            )
        
        # JWT í† í° ìƒì„±
        token = create_jwt_token(request.username)
        expires_at = datetime.utcnow() + timedelta(hours=JWT_EXPIRE_HOURS)
        
        logger.info(f"âœ… ê´€ë¦¬ì ë¡œê·¸ì¸ ì„±ê³µ: {request.username}")
        
        return AdminLoginResponse(
            token=token,
            user=request.username,
            expires_at=expires_at
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¡œê·¸ì¸ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail="ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜")

@app.get("/admin/stats")
async def get_admin_stats():
    """ê´€ë¦¬ììš© ìƒì„¸ í†µê³„ ì •ë³´"""
    try:
        logger.info("ğŸ“Š ê´€ë¦¬ì í†µê³„ ì¡°íšŒ")
        
        # Milvus í†µê³„
        vector_stats = await vector_searcher.get_collection_stats()
        
        # PostgreSQL í†µê³„ (vector_searcherë¥¼ í†µí•´ ì ‘ê·¼)
        doc_stats = await vector_searcher.get_document_stats()
        
        return {
            "total_documents": doc_stats.get("total_documents", 0),
            "total_vectors": vector_stats.get("row_count", 0),
            "indexed_documents": doc_stats.get("indexed_documents", 0),
            "pending_documents": doc_stats.get("pending_documents", 0),
            "last_update": doc_stats.get("last_update", "N/A"),
            "collection_name": os.getenv("COLLECTION_NAME", "uncommon_products"),
            "embedding_dim": int(os.getenv("DIMENSION", 1024)),
            "system_status": "healthy"
        }
        
    except Exception as e:
        logger.error(f"ê´€ë¦¬ì í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/admin/prompt", response_model=SystemPromptResponse)
async def get_system_prompt():
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¡°íšŒ"""
    try:
        logger.info("ğŸ“‹ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¡°íšŒ")
        
        return SystemPromptResponse(
            prompt=SYSTEM_PROMPT,
            updated_at=datetime.utcnow()
        )
        
    except Exception as e:
        logger.error(f"í”„ë¡¬í”„íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/admin/prompt", response_model=SystemPromptResponse)
async def update_system_prompt(request: SystemPromptRequest):
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸"""
    global SYSTEM_PROMPT
    
    try:
        logger.info("ğŸ“ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸")
        
        if not request.prompt.strip():
            raise HTTPException(status_code=400, detail="í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
        
        # í”„ë¡¬í”„íŠ¸ì— í•„ìš”í•œ placeholder í™•ì¸
        if "{query}" not in request.prompt or "{context}" not in request.prompt:
            raise HTTPException(
                status_code=400, 
                detail="í”„ë¡¬í”„íŠ¸ì—ëŠ” {query}ì™€ {context} í”Œë ˆì´ìŠ¤í™€ë”ê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤"
            )
        
        # ë©”ëª¨ë¦¬ì— ì €ì¥ (ì‹¤ì œë¡œëŠ” DBë‚˜ íŒŒì¼ì— ì €ì¥)
        SYSTEM_PROMPT = request.prompt
        updated_at = datetime.utcnow()
        
        logger.info("âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
        return SystemPromptResponse(
            prompt=SYSTEM_PROMPT,
            updated_at=updated_at
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/admin/documents")
async def get_admin_documents():
    """ê´€ë¦¬ììš© ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    try:
        logger.info("ğŸ“š ê´€ë¦¬ì ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ")
        
        # PostgreSQLì—ì„œ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
        documents = await vector_searcher.get_all_documents()
        
        # ë¬¸ì„œë³„ ë²¡í„° ìˆ˜ ì§‘ê³„
        for doc in documents:
            doc["vector_count"] = await vector_searcher.get_document_vector_count(doc["id"])
        
        logger.info(f"ğŸ“Š {len(documents)}ê°œ ë¬¸ì„œ ë°˜í™˜")
        return documents
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/admin/documents")
async def create_admin_document(request: DocumentCreateRequest):
    """ê´€ë¦¬ììš© ë¬¸ì„œ ì¶”ê°€"""
    try:
        logger.info(f"â• ê´€ë¦¬ì ë¬¸ì„œ ì¶”ê°€: {request.title}")
        
        if not request.title.strip() or not request.content.strip():
            raise HTTPException(status_code=400, detail="ì œëª©ê³¼ ë‚´ìš©ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”")
        
        # ë¬¸ì„œë¥¼ PostgreSQLì— ì €ì¥
        doc_id = await vector_searcher.add_manual_document(
            title=request.title,
            content=request.content,
            category=request.category
        )
        
        # ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ì¸ë±ì‹±
        await vector_searcher.index_document(doc_id)
        
        logger.info(f"âœ… ë¬¸ì„œ ì¶”ê°€ ë° ì¸ë±ì‹± ì™„ë£Œ: ID={doc_id}")
        
        return {
            "id": doc_id,
            "title": request.title,
            "message": "ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ê³  ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/admin/documents/{doc_id}")
async def delete_admin_document(doc_id: int):
    """ê´€ë¦¬ììš© ë¬¸ì„œ ì‚­ì œ"""
    try:
        logger.info(f"ğŸ—‘ï¸ ê´€ë¦¬ì ë¬¸ì„œ ì‚­ì œ: ID={doc_id}")
        
        # Milvusì—ì„œ ë²¡í„° ì‚­ì œ
        await vector_searcher.delete_document_vectors(doc_id)
        
        # PostgreSQLì—ì„œ ë¬¸ì„œ ì‚­ì œ
        success = await vector_searcher.delete_document(doc_id)
        
        if not success:
            raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        logger.info(f"âœ… ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: ID={doc_id}")
        
        return {
            "id": doc_id,
            "message": "ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8003)