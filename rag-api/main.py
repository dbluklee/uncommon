"""
UNCOMMON RAG API Service
ì‚¬ìš©ì ì§ˆì˜ë¥¼ ì²˜ë¦¬í•˜ê³  ê´€ë ¨ ì œí’ˆ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
"""

import os
import logging
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException, Depends
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
import json
import asyncio
import time

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from embedding_generator import EmbeddingGenerator
from vector_search import VectorSearcher
from llm_client import LLMClient

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•±
app = FastAPI(
    title="UNCOMMON RAG API Service",
    description="ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ì œí’ˆ ì •ë³´ ì§ˆì˜ì‘ë‹µ ì„œë¹„ìŠ¤",
    version="1.0.0"
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
embedding_generator = None
vector_searcher = None
llm_client = None

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class ChatRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5
    stream: Optional[bool] = True
    temperature: Optional[float] = 0.7
    include_images: Optional[bool] = False
    include_debug: Optional[bool] = False

class ChatResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    query_embedding_dim: int

class SearchRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5

class SearchResponse(BaseModel):
    results: List[Dict[str, Any]]
    query: str
    total_results: int

@app.on_event("startup")
async def startup_event():
    """ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global embedding_generator, vector_searcher, llm_client
    
    try:
        logger.info("ğŸš€ UNCOMMON RAG API ì„œë¹„ìŠ¤ ì‹œì‘")
        
        # ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”
        logger.info("ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        embedding_generator = EmbeddingGenerator()
        logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # ë²¡í„° ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        logger.info("ğŸ”— Milvus ë²¡í„° ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        vector_searcher = VectorSearcher(embedding_generator)
        logger.info("âœ… Milvus ë²¡í„° ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        logger.info("ğŸ¤– Ollama LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        llm_client = LLMClient()
        logger.info("âœ… Ollama LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        logger.info("ğŸ‰ ëª¨ë“  ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        raise

@app.get("/")
async def root():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "service": "UNCOMMON RAG API Service",
        "status": "running",
        "version": "1.0.0",
        "embedding_model": os.getenv("EMBEDDING_MODEL", "BGE-M3"),
        "llm_model": os.getenv("OLLAMA_MODEL", "gemma3"),
        "vector_store": "Milvus"
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy"}

@app.post("/search", response_model=SearchResponse)
async def search_products(request: SearchRequest):
    """
    ì œí’ˆ ë²¡í„° ê²€ìƒ‰ (LLM ì—†ì´ ê²€ìƒ‰ë§Œ)
    """
    try:
        logger.info(f"ğŸ” ê²€ìƒ‰ ìš”ì²­: {request.query}")
        
        # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        search_results = await vector_searcher.search(
            query=request.query,
            top_k=request.top_k
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for result in search_results:
            formatted_results.append({
                "content": result.get("content", ""),
                "product_id": result.get("product_id", 0),
                "product_name": result.get("product_name", ""),
                "chunk_type": result.get("chunk_type", ""),
                "source": result.get("source", ""),
                "score": result.get("score", 0.0)
            })
        
        logger.info(f"âœ… {len(formatted_results)}ê°œ ê²°ê³¼ ë°˜í™˜")
        
        return SearchResponse(
            results=formatted_results,
            query=request.query,
            total_results=len(formatted_results)
        )
        
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat")
async def chat(request: ChatRequest):
    """
    RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
    """
    try:
        start_time = time.time()
        logger.info(f"ğŸ’¬ ì±„íŒ… ìš”ì²­: {request.query}")
        
        # 1. ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        search_start = time.time()
        search_results = await vector_searcher.search(
            query=request.query,
            top_k=request.top_k
        )
        search_end = time.time()
        logger.info(f"â±ï¸ ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {search_end - search_start:.2f}ì´ˆ")
        
        if not search_results:
            logger.warning("âš ï¸ ê´€ë ¨ ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return ChatResponse(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì œí’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                sources=[],
                query_embedding_dim=1024
            )
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_start = time.time()
        context = _build_context(search_results)
        context_end = time.time()
        logger.info(f"â±ï¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {context_end - context_start:.2f}ì´ˆ")
        
        # 3. LLM ì‘ë‹µ ìƒì„±
        if request.stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            logger.info("ğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
            return StreamingResponse(
                _stream_response(request.query, context, search_results, request.temperature, request),
                media_type="text/event-stream"
            )
        else:
            # ì¼ë°˜ ì‘ë‹µ
            llm_start = time.time()
            logger.info("ğŸ¤– LLM ì‘ë‹µ ìƒì„± ì‹œì‘...")
            answer = await llm_client.generate(
                query=request.query,
                context=context,
                temperature=request.temperature
            )
            llm_end = time.time()
            total_end = time.time()
            
            logger.info(f"â±ï¸ LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ: {llm_end - llm_start:.2f}ì´ˆ")
            logger.info(f"â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_end - start_time:.2f}ì´ˆ")
            
            response_data = ChatResponse(
                answer=answer,
                sources=_format_sources(search_results),
                query_embedding_dim=1024
            )
            
            # ë””ë²„ê¹… ì •ë³´ê°€ ìš”ì²­ëœ ê²½ìš° ì¶”ê°€
            if request.include_debug:
                debug_info = _build_debug_info(request.query, search_results, context, request)
                # ChatResponse ëª¨ë¸ì— debug_info í•„ë“œë¥¼ ì¶”ê°€í•˜ê±°ë‚˜, dictë¡œ ë°˜í™˜
                return {
                    "answer": answer,
                    "sources": _format_sources(search_results),
                    "query_embedding_dim": 1024,
                    "debug_info": debug_info
                }
            
            return response_data
            
    except Exception as e:
        logger.error(f"âŒ ì±„íŒ… ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

def _build_context(search_results: List[Dict]) -> str:
    """ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
    context_parts = []
    max_length = int(os.getenv("MAX_CONTEXT_LENGTH", 4000))
    current_length = 0
    
    for i, result in enumerate(search_results, 1):
        content = result.get("content", "")
        product_name = result.get("product_name", "")
        chunk_type = result.get("chunk_type", "")
        
        # ì œí’ˆ ì •ë³´ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context_item = f"[ì œí’ˆì •ë³´ {i}]\n"
        if product_name:
            context_item += f"ì œí’ˆëª…: {product_name}\n"
        if chunk_type:
            context_item += f"ì •ë³´ ìœ í˜•: {chunk_type}\n"
        context_item += f"{content}\n"
        
        # ê¸¸ì´ ì²´í¬
        if current_length + len(context_item) > max_length:
            break
            
        context_parts.append(context_item)
        current_length += len(context_item)
    
    return "\n".join(context_parts)

def _format_sources(search_results: List[Dict]) -> List[Dict]:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì†ŒìŠ¤ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    sources = []
    for result in search_results:
        sources.append({
            "product_name": result.get("product_name", "Unknown"),
            "product_id": result.get("product_id"),
            "chunk_type": result.get("chunk_type"),
            "score": result.get("score", 0.0)
        })
    return sources

def _build_debug_info(query: str, search_results: List[Dict], context: str, request: ChatRequest) -> Dict[str, Any]:
    """ë””ë²„ê¹… ì •ë³´ ìƒì„±"""
    return {
        "query": query,
        "search_results": [
            {
                "product_name": result.get("product_name", "Unknown"),
                "chunk_type": result.get("chunk_type", "unknown"),
                "content": result.get("content", "")[:500],  # ì²˜ìŒ 500ìë§Œ
                "score": result.get("score", 0.0),
                "product_id": result.get("product_id"),
                "source": result.get("source", "")
            }
            for result in search_results
        ],
        "prompt": f"""ë‹¤ìŒì€ UNCOMMON ì•ˆê²½ ì œí’ˆì— ëŒ€í•œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê´€ë ¨ ì œí’ˆ ì •ë³´:
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. 
- ì œí’ˆëª…, ê°€ê²©, íŠ¹ì§•ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
- í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”
- ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ë‹¤ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”""",
        "settings": {
            "top_k": request.top_k,
            "temperature": request.temperature,
            "embedding_model": os.getenv("EMBEDDING_MODEL", "BGE-M3"),
            "llm_model": os.getenv("OLLAMA_MODEL", "gemma3"),
            "stream": request.stream,
            "max_context_length": int(os.getenv("MAX_CONTEXT_LENGTH", 4000))
        }
    }

async def _stream_response(query: str, context: str, search_results: List[Dict], temperature: float, request: ChatRequest = None):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
    try:
        # ë””ë²„ê¹… ì •ë³´ ì¤€ë¹„
        debug_info = None
        if request and request.include_debug:
            debug_info = _build_debug_info(query, search_results, context, request)
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì´ë²¤íŠ¸ (ë””ë²„ê¹… ì •ë³´ í¬í•¨)
        start_data = {
            'type': 'start', 
            'sources': _format_sources(search_results)
        }
        if debug_info:
            start_data['debug_info'] = debug_info
        
        yield f"data: {json.dumps(start_data)}\n\n"
        
        # LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        async for chunk in llm_client.stream_generate(query, context, temperature):
            yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
            await asyncio.sleep(0.01)  # ë°±í”„ë ˆì…” ì œì–´
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì´ë²¤íŠ¸
        yield f"data: {json.dumps({'type': 'end'})}\n\n"
        
    except Exception as e:
        logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}")
        yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

@app.get("/stats")
async def get_stats():
    """ì‹œìŠ¤í…œ í†µê³„ ì •ë³´"""
    try:
        stats = await vector_searcher.get_collection_stats()
        return {
            "collection_name": os.getenv("COLLECTION_NAME", "uncommon_products"),
            "total_vectors": stats.get("row_count", 0),
            "embedding_dim": int(os.getenv("DIMENSION", 1024)),
            "metric_type": os.getenv("METRIC_TYPE", "COSINE")
        }
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return {"error": str(e)}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8003)