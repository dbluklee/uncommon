# RAG LLM ì‹œìŠ¤í…œì˜ ë©”ì¸ API ì„œë¹„ìŠ¤ - ì‚¬ìš©ì ì§ˆì˜ì— ëŒ€í•œ ì¸í…”ë¦¬ì „íŠ¸ ë‹µë³€ ì œê³µ
# ëª©ì : ë²¡í„° ê²€ìƒ‰ + LLMì„ í™œìš©í•œ Retrieval-Augmented Generation ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ
# ê´€ë ¨ í•¨ìˆ˜: chat (ë©”ì¸ RAG), search (ë²¡í„° ê²€ìƒ‰), _stream_rag_response (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)
# ì£¼ìš” ê¸°ëŠ¥: Conditional RAG, ë©€í‹°ëª¨ë‹¬ ì§€ì›, ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°, ë””ë²„ê¹…
"""
UNCOMMON RAG API Service - BGE-M3 ë²¡í„° ê²€ìƒ‰ + Ollama LLM ê¸°ë°˜ ì œí’ˆ ì •ë³´ ì§ˆì˜ì‘ë‹µ
ì£¼ìš” íŠ¹ì§•: Router LLMìœ¼ë¡œ RAG ì‚¬ìš© ì—¬ë¶€ ê²°ì •, ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ, ë©€í‹°ëª¨ë‹¬ ì§€ì›
"""

import os
import logging  # API ìš”ì²­ ë° ì˜¤ë¥˜ ë¡œê¹…
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException, Depends, status, Form, File, UploadFile
from fastapi.responses import StreamingResponse  # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
from fastapi.middleware.cors import CORSMiddleware  # í¬ë¡œìŠ¤ ë„ë©”ì¸ ìš”ì²­ ì²˜ë¦¬
from pydantic import BaseModel  # API ë°ì´í„° ëª¨ë¸ ì •ì˜
from dotenv import load_dotenv  # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
import json
import asyncio  # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
import time
from datetime import datetime

# RAG ì‹œìŠ¤í…œ í•µì‹¬ ëª¨ë“ˆ ì„í¬íŠ¸ - ê°ê° íŠ¹í™”ëœ ê¸°ëŠ¥ ë‹´ë‹¹
from embedding_generator import EmbeddingGenerator  # BGE-M3 ì„ë² ë”© ëª¨ë¸ ê´€ë¦¬
from services.vector_search_service import VectorSearchService  # Milvus ë²¡í„° ê²€ìƒ‰ ì„œë¹„ìŠ¤
from llm_client import LLMClient  # Ollama LLM í´ë¼ì´ì–¸íŠ¸
from router_llm_client import RouterLLMClient  # RAG ì‚¬ìš© ì—¬ë¶€ ê²°ì • ë¼ìš°í„°

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™” - RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ API ì„œë¹„ìŠ¤
app = FastAPI(
    title="UNCOMMON RAG API Service",
    description="ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ì œí’ˆ ì •ë³´ ì§ˆì˜ì‘ë‹µ ì„œë¹„ìŠ¤",
    version="1.0.0"
)

# CORS ì„¤ì • - ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ API ì ‘ê·¼ í—ˆìš©
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # MVPì—ì„œëŠ” ëª¨ë“  ë„ë©”ì¸ í—ˆìš© (í”„ë¡œë•ì…˜ì—ì„œëŠ” ì œí•œ í•„ìš”)
    allow_credentials=True,  # ì¿ í‚¤ ë° ì¸ì¦ í—¤ë” í—ˆìš©
    allow_methods=["*"],  # ëª¨ë“  HTTP ë©”ì†Œë“œ í—ˆìš© (GET, POST, etc.)
    allow_headers=["*"],  # ëª¨ë“  HTTP í—¤ë” í—ˆìš©
)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
embedding_generator = None
vector_search_service = None
llm_client = None
router_llm_client = None

# JWT authentication removed for MVP

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë©”ëª¨ë¦¬ì— ì €ì¥, ì‹¤ì œë¡œëŠ” DBë‚˜ íŒŒì¼ì— ì €ì¥)
SYSTEM_PROMPT = """ë‹¤ìŒì€ UNCOMMON ì•ˆê²½ ì œí’ˆì— ëŒ€í•œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê´€ë ¨ ì œí’ˆ ì •ë³´:
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. 
- ì œí’ˆëª…, ê°€ê²©, íŠ¹ì§•ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
- í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”
- ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ë‹¤ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”"""

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class ChatRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5
    stream: Optional[bool] = True
    temperature: Optional[float] = 0.7
    include_images: Optional[bool] = False
    include_debug: Optional[bool] = False

class MultiModalChatRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5
    stream: Optional[bool] = True
    temperature: Optional[float] = 0.7
    include_debug: Optional[bool] = False

class ChatResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    query_embedding_dim: int

class SearchRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5

class SearchResponse(BaseModel):
    results: List[Dict[str, Any]]
    query: str
    total_results: int

# ê´€ë¦¬ì ëª¨ë¸ë“¤
class AdminLoginRequest(BaseModel):
    username: str
    password: str

class AdminLoginResponse(BaseModel):
    token: str
    user: str
    expires_at: datetime

class SystemPromptRequest(BaseModel):
    prompt: str

class SystemPromptResponse(BaseModel):
    prompt: str
    updated_at: datetime

class DocumentCreateRequest(BaseModel):
    title: str
    content: str
    category: str = "manual"

class DocumentResponse(BaseModel):
    id: int
    title: str
    content: str
    category: str
    created_at: datetime
    vector_count: int

# JWT authentication functions removed for MVP

@app.on_event("startup")
async def startup_event():
    """ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global embedding_generator, vector_search_service, llm_client, router_llm_client
    
    try:
        logger.info("ğŸš€ UNCOMMON RAG API ì„œë¹„ìŠ¤ ì‹œì‘")
        
        # Router LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ê°€ì¥ ë¨¼ì €)
        logger.info("ğŸ¯ Router LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        router_llm_client = RouterLLMClient()
        logger.info("âœ… Router LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”
        logger.info("ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        embedding_generator = EmbeddingGenerator()
        logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # ë²¡í„° ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        logger.info("ğŸ”— ë²¡í„° ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        vector_search_service = VectorSearchService(embedding_generator)
        logger.info("âœ… ë²¡í„° ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        logger.info("ğŸ¤– Ollama LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        llm_client = LLMClient()
        logger.info("âœ… Ollama LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        logger.info("ğŸ‰ ëª¨ë“  ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        raise

@app.get("/")
async def root():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "service": "UNCOMMON RAG API Service",
        "status": "running",
        "version": "1.0.0",
        "embedding_model": os.environ["EMBEDDING_MODEL"],
        "llm_model": os.environ["OLLAMA_MODEL"],
        "vector_store": "Milvus"
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy"}

@app.post("/search", response_model=SearchResponse)
async def search_products(request: SearchRequest):
    """
    ì œí’ˆ ë²¡í„° ê²€ìƒ‰ (LLM ì—†ì´ ê²€ìƒ‰ë§Œ)
    """
    try:
        logger.info(f"ğŸ” ê²€ìƒ‰ ìš”ì²­: {request.query}")
        
        # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        search_results = await vector_search_service.search_similar_documents(
            query=request.query,
            top_k=request.top_k
        )
        
        # ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•íƒœë¡œ ë°˜í™˜ë˜ë¯€ë¡œ í¬ë§·íŒ… ë¶ˆí•„ìš”
        logger.info(f"âœ… {len(search_results)}ê°œ ê²°ê³¼ ë°˜í™˜")
        
        return SearchResponse(
            results=search_results,
            query=request.query,
            total_results=len(search_results)
        )
        
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat")
async def chat(request: ChatRequest):
    """
    Conditional RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
    """
    try:
        start_time = time.time()
        logger.info(f"ğŸ’¬ ì±„íŒ… ìš”ì²­: {request.query}")
        
        # 1. Routerë¡œ RAG í•„ìš”ì„± íŒë‹¨
        router_start = time.time()
        needs_rag = await router_llm_client.should_use_rag(request.query)
        router_end = time.time()
        logger.info(f"â±ï¸ Router íŒë‹¨ ì™„ë£Œ: {router_end - router_start:.2f}ì´ˆ")
        
        if not needs_rag:
            # RAG ì—†ì´ ì§ì ‘ ì‘ë‹µ
            logger.info("ğŸ¯ ì§ì ‘ ì‘ë‹µ ëª¨ë“œ (RAG ë¯¸ì‚¬ìš©)")
            
            if request.stream:
                # ìŠ¤íŠ¸ë¦¬ë° ì§ì ‘ ì‘ë‹µ
                logger.info("ğŸ“¡ ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
                return StreamingResponse(
                    _stream_direct_response(request.query, request.temperature, request),
                    media_type="text/event-stream"
                )
            else:
                # ì¼ë°˜ ì§ì ‘ ì‘ë‹µ
                llm_start = time.time()
                answer = await router_llm_client.generate_direct_response(
                    query=request.query,
                    temperature=request.temperature
                )
                llm_end = time.time()
                total_end = time.time()
                
                logger.info(f"â±ï¸ ì§ì ‘ ì‘ë‹µ ìƒì„± ì™„ë£Œ: {llm_end - llm_start:.2f}ì´ˆ")
                logger.info(f"â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_end - start_time:.2f}ì´ˆ")
                
                response_data = ChatResponse(
                    answer=answer,
                    sources=[],  # ì§ì ‘ ì‘ë‹µì€ ì†ŒìŠ¤ ì—†ìŒ
                    query_embedding_dim=1024
                )
                
                # ë””ë²„ê¹… ì •ë³´ê°€ ìš”ì²­ëœ ê²½ìš° ì¶”ê°€
                if request.include_debug:
                    debug_info = {
                        "query": request.query,
                        "router_decision": "ì§ì ‘ ì‘ë‹µ (RAG ë¯¸ì‚¬ìš©)",
                        "search_results": [],
                        "prompt": f"ì§ì ‘ ì‘ë‹µ í”„ë¡¬í”„íŠ¸ë¡œ ìƒì„±: {request.query}",
                        "settings": {
                            "top_k": request.top_k,
                            "temperature": request.temperature,
                            "router_model": os.environ["ROUTER_LLM_MODEL"],
                            "stream": request.stream
                        }
                    }
                    return {
                        "answer": answer,
                        "sources": [],
                        "query_embedding_dim": 1024,
                        "debug_info": debug_info
                    }
                
                return response_data
        else:
            # RAG ì‚¬ìš©
            logger.info("ğŸ¯ RAG ëª¨ë“œ (ë²¡í„° ê²€ìƒ‰ + ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ)")
            
            # 2. ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            search_start = time.time()
            search_results = await vector_search_service.search_similar_documents(
                query=request.query,
                top_k=request.top_k
            )
            search_end = time.time()
            logger.info(f"â±ï¸ ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {search_end - search_start:.2f}ì´ˆ")
            
            # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_start = time.time()
            if not search_results:
                logger.warning("âš ï¸ ê´€ë ¨ ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                # RAG í•„ìš”í•˜ì§€ë§Œ ë°ì´í„° ì—†ìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ì— "ì •ë³´ ì—†ìŒ"ì„ ëª…ì‹œ
                context = "ê²€ìƒ‰ ê²°ê³¼: ìš”ì²­í•˜ì‹  ì œí’ˆì— ëŒ€í•œ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                search_results = []  # ë¹ˆ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            else:
                context = _build_context(search_results)
            context_end = time.time()
            logger.info(f"â±ï¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {context_end - context_start:.2f}ì´ˆ")
            
            # 4. LLM ì‘ë‹µ ìƒì„± (RAG)
            if request.stream:
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                logger.info("ğŸ“¡ RAG ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
                return StreamingResponse(
                    _stream_rag_response(request.query, context, search_results, request.temperature, request),
                    media_type="text/event-stream"
                )
            else:
                # ì¼ë°˜ ì‘ë‹µ
                llm_start = time.time()
                logger.info("ğŸ¤– RAG LLM ì‘ë‹µ ìƒì„± ì‹œì‘...")
                answer = await llm_client.generate(
                    query=request.query,
                    context=context,
                    temperature=request.temperature
                )
                llm_end = time.time()
                total_end = time.time()
                
                logger.info(f"â±ï¸ RAG LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ: {llm_end - llm_start:.2f}ì´ˆ")
                logger.info(f"â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_end - start_time:.2f}ì´ˆ")
                
                response_data = ChatResponse(
                    answer=answer,
                    sources=_format_sources(search_results),
                    query_embedding_dim=1024
                )
                
                # ë””ë²„ê¹… ì •ë³´ê°€ ìš”ì²­ëœ ê²½ìš° ì¶”ê°€
                if request.include_debug:
                    debug_info = _build_debug_info(request.query, search_results, context, request)
                    debug_info["router_decision"] = "RAG ì‚¬ìš© (ì œí’ˆ ì •ë³´ í•„ìš”)"
                    return {
                        "answer": answer,
                        "sources": _format_sources(search_results),
                        "query_embedding_dim": 1024,
                        "debug_info": debug_info
                    }
                
                return response_data
            
    except Exception as e:
        logger.error(f"âŒ ì±„íŒ… ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat/multimodal")
async def multimodal_chat(
    query: str = Form(...),
    top_k: Optional[int] = Form(5),
    stream: Optional[bool] = Form(True), 
    temperature: Optional[float] = Form(0.7),
    include_debug: Optional[bool] = Form(False),
    image: Optional[UploadFile] = File(None)
):
    """
    ë©€í‹°ëª¨ë‹¬ RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸)
    """
    try:
        start_time = time.time()
        logger.info(f"ğŸ–¼ï¸ ë©€í‹°ëª¨ë‹¬ ì±„íŒ… ìš”ì²­: {query}")
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬
        image_data = None
        if image and image.size > 0:
            # ì´ë¯¸ì§€ í¬ê¸° ì œí•œ (10MB)
            if image.size > 10 * 1024 * 1024:
                raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ í¬ê¸°ê°€ 10MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤")
            
            # ì§€ì›ë˜ëŠ” ì´ë¯¸ì§€ í˜•ì‹ í™•ì¸
            allowed_types = ['image/jpeg', 'image/jpg', 'image/png', 'image/gif', 'image/webp']
            if image.content_type not in allowed_types:
                raise HTTPException(status_code=400, detail="ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤. (JPEG, PNG, GIF, WebP ì§€ì›)")
            
            image_data = await image.read()
            logger.info(f"ğŸ“· ì´ë¯¸ì§€ ì—…ë¡œë“œë¨: {image.filename}, í¬ê¸°: {len(image_data)} bytes, íƒ€ì…: {image.content_type}")
        
        # 1. ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        search_start = time.time()
        search_results = await vector_search_service.search_similar_documents(
            query=query,
            top_k=top_k
        )
        search_end = time.time()
        logger.info(f"â±ï¸ ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {search_end - search_start:.2f}ì´ˆ")
        
        if not search_results:
            logger.warning("âš ï¸ ê´€ë ¨ ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return ChatResponse(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì œí’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                sources=[],
                query_embedding_dim=1024
            )
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_start = time.time()
        context = _build_context(search_results)
        context_end = time.time()
        logger.info(f"â±ï¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {context_end - context_start:.2f}ì´ˆ")
        
        # 3. LLM ì‘ë‹µ ìƒì„± (ì´ë¯¸ì§€ í¬í•¨)
        if stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            logger.info(f"ğŸ“¡ {'ë©€í‹°ëª¨ë‹¬ ' if image_data else ''}ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
            return StreamingResponse(
                _stream_multimodal_response(query, context, search_results, temperature, image_data, include_debug),
                media_type="text/event-stream"
            )
        else:
            # ì¼ë°˜ ì‘ë‹µ
            llm_start = time.time()
            logger.info(f"ğŸ¤– {'ë©€í‹°ëª¨ë‹¬ ' if image_data else ''}LLM ì‘ë‹µ ìƒì„± ì‹œì‘...")
            answer = await llm_client.generate(
                query=query,
                context=context,
                temperature=temperature,
                image_data=image_data
            )
            llm_end = time.time()
            total_end = time.time()
            
            logger.info(f"â±ï¸ LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ: {llm_end - llm_start:.2f}ì´ˆ")
            logger.info(f"â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_end - start_time:.2f}ì´ˆ")
            
            response_data = ChatResponse(
                answer=answer,
                sources=_format_sources(search_results),
                query_embedding_dim=1024
            )
            
            # ë””ë²„ê¹… ì •ë³´ê°€ ìš”ì²­ëœ ê²½ìš° ì¶”ê°€
            if include_debug:
                debug_info = _build_debug_info(query, search_results, context, ChatRequest(
                    query=query, top_k=top_k, temperature=temperature, stream=stream, include_debug=include_debug
                ))
                return {
                    "answer": answer,
                    "sources": _format_sources(search_results),
                    "query_embedding_dim": 1024,
                    "debug_info": debug_info,
                    "has_image": image_data is not None
                }
            
            return response_data
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë©€í‹°ëª¨ë‹¬ ì±„íŒ… ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

def _build_context(search_results: List[Dict]) -> str:
    """ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
    context_parts = []
    max_length = int(os.environ["MAX_CONTEXT_LENGTH"])
    current_length = 0
    
    for i, result in enumerate(search_results, 1):
        content = result.get("content", "")
        product_name = result.get("product_name", "")
        chunk_type = result.get("chunk_type", "")
        
        # ì œí’ˆ ì •ë³´ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context_item = f"[ì œí’ˆì •ë³´ {i}]\n"
        if product_name:
            context_item += f"ì œí’ˆëª…: {product_name}\n"
        if chunk_type:
            context_item += f"ì •ë³´ ìœ í˜•: {chunk_type}\n"
        context_item += f"{content}\n"
        
        # ê¸¸ì´ ì²´í¬
        if current_length + len(context_item) > max_length:
            break
            
        context_parts.append(context_item)
        current_length += len(context_item)
    
    return "\n".join(context_parts)

def _format_sources(search_results: List[Dict]) -> List[Dict]:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì†ŒìŠ¤ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    sources = []
    for result in search_results:
        sources.append({
            "product_name": result.get("product_name", "Unknown"),
            "product_id": result.get("product_id"),
            "chunk_type": result.get("chunk_type"),
            "score": result.get("score", 0.0)
        })
    return sources

def _build_debug_info(query: str, search_results: List[Dict], context: str, request: ChatRequest) -> Dict[str, Any]:
    """ë””ë²„ê¹… ì •ë³´ ìƒì„±"""
    return {
        "query": query,
        "search_results": [
            {
                "product_name": result.get("product_name", "Unknown"),
                "chunk_type": result.get("chunk_type", "unknown"),
                "content": result.get("content", "")[:500],  # ì²˜ìŒ 500ìë§Œ
                "score": result.get("score", 0.0),
                "product_id": result.get("product_id"),
                "source": result.get("source", "")
            }
            for result in search_results
        ],
        "prompt": SYSTEM_PROMPT.format(query=query, context=context),
        "settings": {
            "top_k": request.top_k,
            "temperature": request.temperature,
            "embedding_model": os.environ["EMBEDDING_MODEL"],
            "llm_model": os.environ["OLLAMA_MODEL"],
            "stream": request.stream,
            "max_context_length": int(os.environ["MAX_CONTEXT_LENGTH"])
        }
    }

async def _stream_direct_response(query: str, temperature: float, request: ChatRequest = None):
    """ì§ì ‘ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°"""
    try:
        # ë””ë²„ê¹… ì •ë³´ ì¤€ë¹„
        debug_info = None
        if request and request.include_debug:
            debug_info = {
                "query": query,
                "router_decision": "ì§ì ‘ ì‘ë‹µ (RAG ë¯¸ì‚¬ìš©)",
                "search_results": [],
                "prompt": f"ì§ì ‘ ì‘ë‹µ í”„ë¡¬í”„íŠ¸ë¡œ ìƒì„±: {query}",
                "settings": {
                    "top_k": request.top_k,
                    "temperature": temperature,
                    "router_model": os.environ["ROUTER_LLM_MODEL"],
                    "stream": request.stream
                }
            }
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì´ë²¤íŠ¸
        start_data = {
            'type': 'start', 
            'sources': []
        }
        if debug_info:
            start_data['debug_info'] = debug_info
        
        yield f"data: {json.dumps(start_data)}\n\n"
        
        # Router LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        async for chunk in router_llm_client.stream_direct_response(query, temperature):
            yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
            await asyncio.sleep(0.01)
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì´ë²¤íŠ¸
        yield f"data: {json.dumps({'type': 'end'})}\n\n"
        
    except Exception as e:
        logger.error(f"ì§ì ‘ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}")
        yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

async def _stream_rag_response(query: str, context: str, search_results: List[Dict], temperature: float, request: ChatRequest = None):
    """RAG ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
    try:
        # ë””ë²„ê¹… ì •ë³´ ì¤€ë¹„
        debug_info = None
        if request and request.include_debug:
            debug_info = _build_debug_info(query, search_results, context, request)
            debug_info["router_decision"] = "RAG ì‚¬ìš© (ì œí’ˆ ì •ë³´ í•„ìš”)"
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì´ë²¤íŠ¸ (ë””ë²„ê¹… ì •ë³´ í¬í•¨)
        start_data = {
            'type': 'start', 
            'sources': _format_sources(search_results)
        }
        if debug_info:
            start_data['debug_info'] = debug_info
        
        yield f"data: {json.dumps(start_data)}\n\n"
        
        # LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        async for chunk in llm_client.stream_generate(query, context, temperature):
            yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
            await asyncio.sleep(0.01)  # ë°±í”„ë ˆì…” ì œì–´
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì´ë²¤íŠ¸
        yield f"data: {json.dumps({'type': 'end'})}\n\n"
        
    except Exception as e:
        logger.error(f"RAG ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}")
        yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

async def _stream_multimodal_response(query: str, context: str, search_results: List[Dict], temperature: float, image_data: Optional[bytes] = None, include_debug: bool = False):
    """ë©€í‹°ëª¨ë‹¬ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
    try:
        # ë””ë²„ê¹… ì •ë³´ ì¤€ë¹„
        debug_info = None
        if include_debug:
            debug_info = _build_debug_info(query, search_results, context, ChatRequest(
                query=query, temperature=temperature, stream=True, include_debug=include_debug
            ))
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì´ë²¤íŠ¸ (ë””ë²„ê¹… ì •ë³´ í¬í•¨)
        start_data = {
            'type': 'start', 
            'sources': _format_sources(search_results),
            'has_image': image_data is not None
        }
        if debug_info:
            start_data['debug_info'] = debug_info
        
        yield f"data: {json.dumps(start_data)}\n\n"
        
        # LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ì´ë¯¸ì§€ í¬í•¨)
        async for chunk in llm_client.stream_generate(query, context, temperature, image_data):
            yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
            await asyncio.sleep(0.01)  # ë°±í”„ë ˆì…” ì œì–´
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì´ë²¤íŠ¸
        yield f"data: {json.dumps({'type': 'end'})}\n\n"
        
    except Exception as e:
        logger.error(f"ë©€í‹°ëª¨ë‹¬ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}")
        yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

@app.get("/stats")
async def get_stats():
    """ì‹œìŠ¤í…œ í†µê³„ ì •ë³´"""
    try:
        stats = await vector_search_service.get_service_stats()
        return {
            "collection_name": os.environ["COLLECTION_NAME"],
            "total_vectors": stats.get("vector_store", {}).get("row_count", 0),
            "embedding_dim": int(os.environ["DIMENSION"]),
            "metric_type": os.environ["METRIC_TYPE"]
        }
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return {"error": str(e)}

# Admin endpoints for management dashboard
@app.get("/admin/stats")
async def get_admin_stats():
    """ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ í†µê³„"""
    try:
        stats = await vector_search_service.get_service_stats()
        return {
            "total_documents": 0,  # MVPì—ì„œëŠ” ì œí’ˆ ê¸°ë°˜ì´ë¯€ë¡œ 0
            "total_vectors": stats.get("vector_store", {}).get("row_count", 0),
            "last_update": "ì‹¤ì‹œê°„"
        }
    except Exception as e:
        logger.error(f"ê´€ë¦¬ì í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return {
            "total_documents": 0,
            "total_vectors": 0,
            "last_update": "ì˜¤ë¥˜"
        }

@app.get("/admin/prompt")
async def get_system_prompt():
    """í˜„ì¬ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¡°íšŒ"""
    return {"prompt": SYSTEM_PROMPT}

@app.post("/admin/prompt")
async def update_system_prompt(request: SystemPromptRequest):
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸"""
    global SYSTEM_PROMPT
    SYSTEM_PROMPT = request.prompt
    logger.info("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì—…ë°ì´íŠ¸ë¨")
    return {
        "prompt": SYSTEM_PROMPT,
        "updated_at": datetime.now()
    }

@app.get("/admin/documents")
async def get_documents():
    """ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ (MVPì—ì„œëŠ” ì œí’ˆ ë°ì´í„° ë°˜í™˜)"""
    try:
        # PostgreSQLì—ì„œ ì œí’ˆ ë°ì´í„° ì¡°íšŒ
        import psycopg2
        import json
        
        conn = psycopg2.connect(
            host=os.environ["POSTGRES_HOST"],
            port=os.environ["POSTGRES_PORT"], 
            database=os.environ["POSTGRES_DB"],
            user=os.environ["POSTGRES_USER"],
            password=os.environ["POSTGRES_PASSWORD"]
        )
        
        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT id, product_name, description, indexed, indexed_at
                FROM products
                WHERE indexed = true
                ORDER BY indexed_at DESC
            """)
            results = cursor.fetchall()
        
        conn.close()
        
        documents = []
        for row in results:
            documents.append({
                "id": row[0],
                "title": row[1] or f"ì œí’ˆ {row[0]}",
                "product_name": row[1],
                "category": "product",
                "chunk_type": "product_info",
                "vector_count": 1,
                "created_at": row[4] or datetime.now()
            })
        
        return documents
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return []

@app.post("/admin/documents")
async def create_document(request: DocumentCreateRequest):
    """ìƒˆ ë¬¸ì„œ ì¶”ê°€ (MVPì—ì„œëŠ” ë¯¸êµ¬í˜„)"""
    # MVPì—ì„œëŠ” ì œí’ˆ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ìˆ˜ë™ ë¬¸ì„œ ì¶”ê°€ëŠ” ë¯¸êµ¬í˜„
    raise HTTPException(status_code=501, detail="MVPì—ì„œëŠ” ì œí’ˆ ë°ì´í„°ë§Œ ì§€ì›ë©ë‹ˆë‹¤")

@app.delete("/admin/documents/{doc_id}")
async def delete_document(doc_id: int):
    """ë¬¸ì„œ ì‚­ì œ (MVPì—ì„œëŠ” ë¯¸êµ¬í˜„)"""
    # MVPì—ì„œëŠ” ì œí’ˆ ë°ì´í„° ì‚­ì œëŠ” ìœ„í—˜í•˜ë¯€ë¡œ ë¯¸êµ¬í˜„
    raise HTTPException(status_code=501, detail="MVPì—ì„œëŠ” ì œí’ˆ ë°ì´í„° ì‚­ì œê°€ ì œí•œë©ë‹ˆë‹¤")

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ["RAG_API_INTERNAL_PORT"])
    uvicorn.run(app, host="0.0.0.0", port=port)