"""
UNCOMMON RAG API Service
ì‚¬ìš©ì ì§ˆì˜ë¥¼ ì²˜ë¦¬í•˜ê³  ê´€ë ¨ ì œí’ˆ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
"""

import os
import logging
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException, Depends, status
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
import json
import asyncio
import time
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from embedding_generator import EmbeddingGenerator
from vector_search import VectorSearcher
from llm_client import LLMClient

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•±
app = FastAPI(
    title="UNCOMMON RAG API Service",
    description="ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ì œí’ˆ ì •ë³´ ì§ˆì˜ì‘ë‹µ ì„œë¹„ìŠ¤",
    version="1.0.0"
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
embedding_generator = None
vector_searcher = None
llm_client = None

# JWT authentication removed for MVP

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë©”ëª¨ë¦¬ì— ì €ì¥, ì‹¤ì œë¡œëŠ” DBë‚˜ íŒŒì¼ì— ì €ì¥)
SYSTEM_PROMPT = """ë‹¤ìŒì€ UNCOMMON ì•ˆê²½ ì œí’ˆì— ëŒ€í•œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê´€ë ¨ ì œí’ˆ ì •ë³´:
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. 
- ì œí’ˆëª…, ê°€ê²©, íŠ¹ì§•ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
- í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”
- ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ë‹¤ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”"""

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class ChatRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5
    stream: Optional[bool] = True
    temperature: Optional[float] = 0.7
    include_images: Optional[bool] = False
    include_debug: Optional[bool] = False

class MultiModalChatRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5
    stream: Optional[bool] = True
    temperature: Optional[float] = 0.7
    include_debug: Optional[bool] = False

class ChatResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    query_embedding_dim: int

class SearchRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5

class SearchResponse(BaseModel):
    results: List[Dict[str, Any]]
    query: str
    total_results: int

# ê´€ë¦¬ì ëª¨ë¸ë“¤
class AdminLoginRequest(BaseModel):
    username: str
    password: str

class AdminLoginResponse(BaseModel):
    token: str
    user: str
    expires_at: datetime

class SystemPromptRequest(BaseModel):
    prompt: str

class SystemPromptResponse(BaseModel):
    prompt: str
    updated_at: datetime

class DocumentCreateRequest(BaseModel):
    title: str
    content: str
    category: str = "manual"

class DocumentResponse(BaseModel):
    id: int
    title: str
    content: str
    category: str
    created_at: datetime
    vector_count: int

# JWT authentication functions removed for MVP

@app.on_event("startup")
async def startup_event():
    """ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global embedding_generator, vector_searcher, llm_client
    
    try:
        logger.info("ğŸš€ UNCOMMON RAG API ì„œë¹„ìŠ¤ ì‹œì‘")
        
        # ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”
        logger.info("ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        embedding_generator = EmbeddingGenerator()
        logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # ë²¡í„° ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        logger.info("ğŸ”— Milvus ë²¡í„° ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        vector_searcher = VectorSearcher(embedding_generator)
        logger.info("âœ… Milvus ë²¡í„° ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        logger.info("ğŸ¤– Ollama LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        llm_client = LLMClient()
        logger.info("âœ… Ollama LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        logger.info("ğŸ‰ ëª¨ë“  ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        raise

@app.get("/")
async def root():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "service": "UNCOMMON RAG API Service",
        "status": "running",
        "version": "1.0.0",
        "embedding_model": os.environ["EMBEDDING_MODEL"],
        "llm_model": os.environ["OLLAMA_MODEL"],
        "vector_store": "Milvus"
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy"}

@app.post("/search", response_model=SearchResponse)
async def search_products(request: SearchRequest):
    """
    ì œí’ˆ ë²¡í„° ê²€ìƒ‰ (LLM ì—†ì´ ê²€ìƒ‰ë§Œ)
    """
    try:
        logger.info(f"ğŸ” ê²€ìƒ‰ ìš”ì²­: {request.query}")
        
        # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        search_results = await vector_searcher.search(
            query=request.query,
            top_k=request.top_k
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for result in search_results:
            formatted_results.append({
                "content": result.get("content", ""),
                "product_id": result.get("product_id", 0),
                "product_name": result.get("product_name", ""),
                "chunk_type": result.get("chunk_type", ""),
                "source": result.get("source", ""),
                "score": result.get("score", 0.0)
            })
        
        logger.info(f"âœ… {len(formatted_results)}ê°œ ê²°ê³¼ ë°˜í™˜")
        
        return SearchResponse(
            results=formatted_results,
            query=request.query,
            total_results=len(formatted_results)
        )
        
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat")
async def chat(request: ChatRequest):
    """
    RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
    """
    try:
        start_time = time.time()
        logger.info(f"ğŸ’¬ ì±„íŒ… ìš”ì²­: {request.query}")
        
        # 1. ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        search_start = time.time()
        search_results = await vector_searcher.search(
            query=request.query,
            top_k=request.top_k
        )
        search_end = time.time()
        logger.info(f"â±ï¸ ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {search_end - search_start:.2f}ì´ˆ")
        
        if not search_results:
            logger.warning("âš ï¸ ê´€ë ¨ ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return ChatResponse(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì œí’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                sources=[],
                query_embedding_dim=1024
            )
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_start = time.time()
        context = _build_context(search_results)
        context_end = time.time()
        logger.info(f"â±ï¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {context_end - context_start:.2f}ì´ˆ")
        
        # 3. LLM ì‘ë‹µ ìƒì„±
        if request.stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            logger.info("ğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
            return StreamingResponse(
                _stream_response(request.query, context, search_results, request.temperature, request),
                media_type="text/event-stream"
            )
        else:
            # ì¼ë°˜ ì‘ë‹µ
            llm_start = time.time()
            logger.info("ğŸ¤– LLM ì‘ë‹µ ìƒì„± ì‹œì‘...")
            answer = await llm_client.generate(
                query=request.query,
                context=context,
                temperature=request.temperature
            )
            llm_end = time.time()
            total_end = time.time()
            
            logger.info(f"â±ï¸ LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ: {llm_end - llm_start:.2f}ì´ˆ")
            logger.info(f"â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_end - start_time:.2f}ì´ˆ")
            
            response_data = ChatResponse(
                answer=answer,
                sources=_format_sources(search_results),
                query_embedding_dim=1024
            )
            
            # ë””ë²„ê¹… ì •ë³´ê°€ ìš”ì²­ëœ ê²½ìš° ì¶”ê°€
            if request.include_debug:
                debug_info = _build_debug_info(request.query, search_results, context, request)
                # ChatResponse ëª¨ë¸ì— debug_info í•„ë“œë¥¼ ì¶”ê°€í•˜ê±°ë‚˜, dictë¡œ ë°˜í™˜
                return {
                    "answer": answer,
                    "sources": _format_sources(search_results),
                    "query_embedding_dim": 1024,
                    "debug_info": debug_info
                }
            
            return response_data
            
    except Exception as e:
        logger.error(f"âŒ ì±„íŒ… ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat/multimodal")
async def multimodal_chat(
    query: str = Form(...),
    top_k: Optional[int] = Form(5),
    stream: Optional[bool] = Form(True), 
    temperature: Optional[float] = Form(0.7),
    include_debug: Optional[bool] = Form(False),
    image: Optional[UploadFile] = File(None)
):
    """
    ë©€í‹°ëª¨ë‹¬ RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸)
    """
    try:
        start_time = time.time()
        logger.info(f"ğŸ–¼ï¸ ë©€í‹°ëª¨ë‹¬ ì±„íŒ… ìš”ì²­: {query}")
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬
        image_data = None
        if image and image.size > 0:
            # ì´ë¯¸ì§€ í¬ê¸° ì œí•œ (10MB)
            if image.size > 10 * 1024 * 1024:
                raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ í¬ê¸°ê°€ 10MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤")
            
            # ì§€ì›ë˜ëŠ” ì´ë¯¸ì§€ í˜•ì‹ í™•ì¸
            allowed_types = ['image/jpeg', 'image/jpg', 'image/png', 'image/gif', 'image/webp']
            if image.content_type not in allowed_types:
                raise HTTPException(status_code=400, detail="ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤. (JPEG, PNG, GIF, WebP ì§€ì›)")
            
            image_data = await image.read()
            logger.info(f"ğŸ“· ì´ë¯¸ì§€ ì—…ë¡œë“œë¨: {image.filename}, í¬ê¸°: {len(image_data)} bytes, íƒ€ì…: {image.content_type}")
        
        # 1. ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        search_start = time.time()
        search_results = await vector_searcher.search(
            query=query,
            top_k=top_k
        )
        search_end = time.time()
        logger.info(f"â±ï¸ ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {search_end - search_start:.2f}ì´ˆ")
        
        if not search_results:
            logger.warning("âš ï¸ ê´€ë ¨ ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return ChatResponse(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì œí’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                sources=[],
                query_embedding_dim=1024
            )
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_start = time.time()
        context = _build_context(search_results)
        context_end = time.time()
        logger.info(f"â±ï¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {context_end - context_start:.2f}ì´ˆ")
        
        # 3. LLM ì‘ë‹µ ìƒì„± (ì´ë¯¸ì§€ í¬í•¨)
        if stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            logger.info(f"ğŸ“¡ {'ë©€í‹°ëª¨ë‹¬ ' if image_data else ''}ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
            return StreamingResponse(
                _stream_multimodal_response(query, context, search_results, temperature, image_data, include_debug),
                media_type="text/event-stream"
            )
        else:
            # ì¼ë°˜ ì‘ë‹µ
            llm_start = time.time()
            logger.info(f"ğŸ¤– {'ë©€í‹°ëª¨ë‹¬ ' if image_data else ''}LLM ì‘ë‹µ ìƒì„± ì‹œì‘...")
            answer = await llm_client.generate(
                query=query,
                context=context,
                temperature=temperature,
                image_data=image_data
            )
            llm_end = time.time()
            total_end = time.time()
            
            logger.info(f"â±ï¸ LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ: {llm_end - llm_start:.2f}ì´ˆ")
            logger.info(f"â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_end - start_time:.2f}ì´ˆ")
            
            response_data = ChatResponse(
                answer=answer,
                sources=_format_sources(search_results),
                query_embedding_dim=1024
            )
            
            # ë””ë²„ê¹… ì •ë³´ê°€ ìš”ì²­ëœ ê²½ìš° ì¶”ê°€
            if include_debug:
                debug_info = _build_debug_info(query, search_results, context, ChatRequest(
                    query=query, top_k=top_k, temperature=temperature, stream=stream, include_debug=include_debug
                ))
                return {
                    "answer": answer,
                    "sources": _format_sources(search_results),
                    "query_embedding_dim": 1024,
                    "debug_info": debug_info,
                    "has_image": image_data is not None
                }
            
            return response_data
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë©€í‹°ëª¨ë‹¬ ì±„íŒ… ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

def _build_context(search_results: List[Dict]) -> str:
    """ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
    context_parts = []
    max_length = int(os.environ["MAX_CONTEXT_LENGTH"])
    current_length = 0
    
    for i, result in enumerate(search_results, 1):
        content = result.get("content", "")
        product_name = result.get("product_name", "")
        chunk_type = result.get("chunk_type", "")
        
        # ì œí’ˆ ì •ë³´ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context_item = f"[ì œí’ˆì •ë³´ {i}]\n"
        if product_name:
            context_item += f"ì œí’ˆëª…: {product_name}\n"
        if chunk_type:
            context_item += f"ì •ë³´ ìœ í˜•: {chunk_type}\n"
        context_item += f"{content}\n"
        
        # ê¸¸ì´ ì²´í¬
        if current_length + len(context_item) > max_length:
            break
            
        context_parts.append(context_item)
        current_length += len(context_item)
    
    return "\n".join(context_parts)

def _format_sources(search_results: List[Dict]) -> List[Dict]:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì†ŒìŠ¤ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    sources = []
    for result in search_results:
        sources.append({
            "product_name": result.get("product_name", "Unknown"),
            "product_id": result.get("product_id"),
            "chunk_type": result.get("chunk_type"),
            "score": result.get("score", 0.0)
        })
    return sources

def _build_debug_info(query: str, search_results: List[Dict], context: str, request: ChatRequest) -> Dict[str, Any]:
    """ë””ë²„ê¹… ì •ë³´ ìƒì„±"""
    return {
        "query": query,
        "search_results": [
            {
                "product_name": result.get("product_name", "Unknown"),
                "chunk_type": result.get("chunk_type", "unknown"),
                "content": result.get("content", "")[:500],  # ì²˜ìŒ 500ìë§Œ
                "score": result.get("score", 0.0),
                "product_id": result.get("product_id"),
                "source": result.get("source", "")
            }
            for result in search_results
        ],
        "prompt": SYSTEM_PROMPT.format(query=query, context=context),
        "settings": {
            "top_k": request.top_k,
            "temperature": request.temperature,
            "embedding_model": os.environ["EMBEDDING_MODEL"],
            "llm_model": os.environ["OLLAMA_MODEL"],
            "stream": request.stream,
            "max_context_length": int(os.environ["MAX_CONTEXT_LENGTH"])
        }
    }

async def _stream_response(query: str, context: str, search_results: List[Dict], temperature: float, request: ChatRequest = None):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
    try:
        # ë””ë²„ê¹… ì •ë³´ ì¤€ë¹„
        debug_info = None
        if request and request.include_debug:
            debug_info = _build_debug_info(query, search_results, context, request)
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì´ë²¤íŠ¸ (ë””ë²„ê¹… ì •ë³´ í¬í•¨)
        start_data = {
            'type': 'start', 
            'sources': _format_sources(search_results)
        }
        if debug_info:
            start_data['debug_info'] = debug_info
        
        yield f"data: {json.dumps(start_data)}\n\n"
        
        # LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        async for chunk in llm_client.stream_generate(query, context, temperature):
            yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
            await asyncio.sleep(0.01)  # ë°±í”„ë ˆì…” ì œì–´
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì´ë²¤íŠ¸
        yield f"data: {json.dumps({'type': 'end'})}\n\n"
        
    except Exception as e:
        logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}")
        yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

async def _stream_multimodal_response(query: str, context: str, search_results: List[Dict], temperature: float, image_data: Optional[bytes] = None, include_debug: bool = False):
    """ë©€í‹°ëª¨ë‹¬ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
    try:
        # ë””ë²„ê¹… ì •ë³´ ì¤€ë¹„
        debug_info = None
        if include_debug:
            debug_info = _build_debug_info(query, search_results, context, ChatRequest(
                query=query, temperature=temperature, stream=True, include_debug=include_debug
            ))
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì´ë²¤íŠ¸ (ë””ë²„ê¹… ì •ë³´ í¬í•¨)
        start_data = {
            'type': 'start', 
            'sources': _format_sources(search_results),
            'has_image': image_data is not None
        }
        if debug_info:
            start_data['debug_info'] = debug_info
        
        yield f"data: {json.dumps(start_data)}\n\n"
        
        # LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ì´ë¯¸ì§€ í¬í•¨)
        async for chunk in llm_client.stream_generate(query, context, temperature, image_data):
            yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
            await asyncio.sleep(0.01)  # ë°±í”„ë ˆì…” ì œì–´
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì´ë²¤íŠ¸
        yield f"data: {json.dumps({'type': 'end'})}\n\n"
        
    except Exception as e:
        logger.error(f"ë©€í‹°ëª¨ë‹¬ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}")
        yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

@app.get("/stats")
async def get_stats():
    """ì‹œìŠ¤í…œ í†µê³„ ì •ë³´"""
    try:
        stats = await vector_searcher.get_collection_stats()
        return {
            "collection_name": os.environ["COLLECTION_NAME"],
            "total_vectors": stats.get("row_count", 0),
            "embedding_dim": int(os.environ["DIMENSION"]),
            "metric_type": os.environ["METRIC_TYPE"]
        }
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return {"error": str(e)}

# Admin endpoints removed for MVP

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ["RAG_API_INTERNAL_PORT"])
    uvicorn.run(app, host="0.0.0.0", port=port)