"""
LLM í´ë¼ì´ì–¸íŠ¸ ëª¨ë“ˆ
Ollama Gemma3 ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±
"""

import os
import logging
import requests
import json
from typing import AsyncGenerator, Optional, List, Dict, Any
import aiohttp
import asyncio
import base64
from PIL import Image
import io

logger = logging.getLogger(__name__)

class LLMClient:
    """Ollama LLM í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.ollama_host = os.environ["OLLAMA_HOST"]
        self.ollama_port = os.environ["OLLAMA_PORT"]
        self.model_name = os.environ["OLLAMA_MODEL"]
        # Ensure we use the full model name
        if self.model_name == "gemma3":
            self.model_name = "gemma3:27b-it-q4_K_M"
        
        # API ì—”ë“œí¬ì¸íŠ¸
        self.base_url = f"http://{self.ollama_host}:{self.ollama_port}"
        self.generate_url = f"{self.base_url}/api/generate"
        self.chat_url = f"{self.base_url}/api/chat"
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        self._test_connection()
    
    def _test_connection(self):
        """Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [m.get("name", "") for m in models]
                
                logger.info(f"âœ… Ollama ì—°ê²° ì„±ê³µ! ì„¤ì • ëª¨ë¸: {self.model_name}")
                if any(self.model_name in name for name in model_names):
                    logger.info(f"âœ… ëª¨ë¸ '{self.model_name}' í™•ì¸ë¨")
                else:
                    logger.warning(f"âš ï¸ ëª¨ë¸ '{self.model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {model_names}")
                    # ì‚¬ìš© ê°€ëŠ¥í•œ gemma3 ëª¨ë¸ ìë™ ì„ íƒ
                    gemma3_models = [name for name in model_names if 'gemma3' in name.lower()]
                    if gemma3_models:
                        self.model_name = gemma3_models[0]
                        logger.info(f"ğŸ”„ ìë™ ì„ íƒëœ ëª¨ë¸: {self.model_name}")
            else:
                logger.warning(f"âš ï¸ Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                
        except Exception as e:
            logger.error(f"âŒ Ollama ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            logger.info("ë¡œì»¬ Ollamaë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì›ê²© ì„œë²„ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”")
    
    def _build_prompt(self, query: str, context: str, has_image: bool = False) -> str:
        """í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        if has_image:
            prompt = f"""ë‹¹ì‹ ì€ UNCOMMON ì•ˆê²½ ë¸Œëœë“œì˜ ì œí’ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ì´ë¯¸ì§€ì™€ ì œí’ˆ ì •ë³´ë¥¼ í•¨ê»˜ ë¶„ì„í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì œí’ˆ ì •ë³´:
{context}

ê³ ê° ì§ˆë¬¸: {query}

ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ê³ , ì œí’ˆ ì •ë³´ì™€ í•¨ê»˜ ê³ ë ¤í•´ì„œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ë‹µë³€:"""
        else:
            prompt = f"""ë‹¹ì‹ ì€ UNCOMMON ì•ˆê²½ ë¸Œëœë“œì˜ ì œí’ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì œê³µëœ ì œí’ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ ê°ì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì œí’ˆ ì •ë³´:
{context}

ê³ ê° ì§ˆë¬¸: {query}

ë‹µë³€:"""
        return prompt
    
    def _encode_image(self, image_data: bytes) -> str:
        """ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©"""
        return base64.b64encode(image_data).decode('utf-8')
    
    def _process_image(self, image_data: bytes, max_size: tuple = (1024, 1024)) -> bytes:
        """ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • ë° ìµœì í™”"""
        try:
            # PIL Imageë¡œ ë³€í™˜
            image = Image.open(io.BytesIO(image_data))
            
            # RGBA -> RGB ë³€í™˜ (JPEG í˜¸í™˜ì„±)
            if image.mode in ('RGBA', 'LA'):
                background = Image.new('RGB', image.size, (255, 255, 255))
                if image.mode == 'LA':
                    image = image.convert('RGBA')
                background.paste(image, mask=image.split()[-1])
                image = background
            elif image.mode != 'RGB':
                image = image.convert('RGB')
            
            # í¬ê¸° ì¡°ì •
            image.thumbnail(max_size, Image.Resampling.LANCZOS)
            
            # JPEGë¡œ ì••ì¶•
            output = io.BytesIO()
            image.save(output, format='JPEG', quality=85, optimize=True)
            return output.getvalue()
            
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return image_data  # ì›ë³¸ ë°˜í™˜
    
    async def generate(self, query: str, context: str, temperature: float = 0.7, image_data: Optional[bytes] = None) -> str:
        """
        ë™ê¸°ì‹ ì‘ë‹µ ìƒì„± (ì´ë¯¸ì§€ ì§€ì›)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
            temperature: ìƒì„± ì˜¨ë„
            image_data: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„° (ì„ íƒì‚¬í•­)
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        try:
            # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ì±„íŒ… API ì‚¬ìš©
            if image_data:
                return await self._generate_with_image(query, context, temperature, image_data, stream=False)
            
            # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš° ê¸°ì¡´ ë°©ì‹
            prompt = self._build_prompt(query, context)
            
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "temperature": temperature,
                "stream": False
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(self.generate_url, json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result.get("response", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        error_text = await response.text()
                        logger.error(f"Ollama API ì˜¤ë¥˜: {response.status} - {error_text}")
                        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def _generate_with_image(self, query: str, context: str, temperature: float, image_data: bytes, stream: bool = False) -> str:
        """
        ì´ë¯¸ì§€ë¥¼ í¬í•¨í•œ ì‘ë‹µ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
            temperature: ìƒì„± ì˜¨ë„
            image_data: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
            stream: ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        try:
            # ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì¸ì½”ë”©
            processed_image = self._process_image(image_data)
            image_b64 = self._encode_image(processed_image)
            
            # ì±„íŒ… ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                {
                    "role": "user",
                    "content": self._build_prompt(query, context, has_image=True),
                    "images": [image_b64]
                }
            ]
            
            payload = {
                "model": self.model_name,
                "messages": messages,
                "temperature": temperature,
                "stream": stream
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(self.chat_url, json=payload) as response:
                    if response.status == 200:
                        if stream:
                            # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ëŠ” ë³„ë„ ë©”ì†Œë“œì—ì„œ
                            return ""
                        else:
                            result = await response.json()
                            return result.get("message", {}).get("content", "ì´ë¯¸ì§€ ë¶„ì„ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        error_text = await response.text()
                        logger.error(f"Ollama ì´ë¯¸ì§€ API ì˜¤ë¥˜: {response.status} - {error_text}")
                        return "ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return f"ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def stream_generate(self, query: str, context: str, temperature: float = 0.7, image_data: Optional[bytes] = None) -> AsyncGenerator[str, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (ì´ë¯¸ì§€ ì§€ì›)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
            temperature: ìƒì„± ì˜¨ë„
            image_data: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„° (ì„ íƒì‚¬í•­)
            
        Yields:
            ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬
        """
        try:
            # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ë©€í‹°ëª¨ë‹¬ ìŠ¤íŠ¸ë¦¬ë°
            if image_data:
                async for chunk in self._stream_with_image(query, context, temperature, image_data):
                    yield chunk
                return
            
            # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš° ê¸°ì¡´ ë°©ì‹
            prompt = self._build_prompt(query, context)
            
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "temperature": temperature,
                "stream": True
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(self.generate_url, json=payload) as response:
                    if response.status == 200:
                        async for line in response.content:
                            if line:
                                try:
                                    # NDJSON íŒŒì‹±
                                    data = json.loads(line.decode('utf-8'))
                                    if "response" in data:
                                        yield data["response"]
                                    
                                    # ì¢…ë£Œ í™•ì¸
                                    if data.get("done", False):
                                        break
                                        
                                except json.JSONDecodeError:
                                    continue
                                except Exception as e:
                                    logger.error(f"ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                                    continue
                    else:
                        error_text = await response.text()
                        logger.error(f"Ollama ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {response.status} - {error_text}")
                        yield "ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì‹¤íŒ¨: {str(e)}")
            yield f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}"
    
    async def _stream_with_image(self, query: str, context: str, temperature: float, image_data: bytes) -> AsyncGenerator[str, None]:
        """
        ì´ë¯¸ì§€ë¥¼ í¬í•¨í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
            temperature: ìƒì„± ì˜¨ë„
            image_data: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
            
        Yields:
            ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬
        """
        try:
            # ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì¸ì½”ë”©
            processed_image = self._process_image(image_data)
            image_b64 = self._encode_image(processed_image)
            
            # ì±„íŒ… ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                {
                    "role": "user",
                    "content": self._build_prompt(query, context, has_image=True),
                    "images": [image_b64]
                }
            ]
            
            payload = {
                "model": self.model_name,
                "messages": messages,
                "temperature": temperature,
                "stream": True
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(self.chat_url, json=payload) as response:
                    if response.status == 200:
                        async for line in response.content:
                            if line:
                                try:
                                    # NDJSON íŒŒì‹±
                                    data = json.loads(line.decode('utf-8'))
                                    if "message" in data and "content" in data["message"]:
                                        yield data["message"]["content"]
                                    
                                    # ì¢…ë£Œ í™•ì¸
                                    if data.get("done", False):
                                        break
                                        
                                except json.JSONDecodeError:
                                    continue
                                except Exception as e:
                                    logger.error(f"ì´ë¯¸ì§€ ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                                    continue
                    else:
                        error_text = await response.text()
                        logger.error(f"Ollama ì´ë¯¸ì§€ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {response.status} - {error_text}")
                        yield "ì´ë¯¸ì§€ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì‹¤íŒ¨: {str(e)}")
            yield f"ì´ë¯¸ì§€ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}"
    
    async def chat(self, messages: list, temperature: float = 0.7) -> str:
        """
        ì±„íŒ… í˜•ì‹ ì‘ë‹µ ìƒì„±
        
        Args:
            messages: ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            temperature: ìƒì„± ì˜¨ë„
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        try:
            payload = {
                "model": self.model_name,
                "messages": messages,
                "temperature": temperature,
                "stream": False
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(self.chat_url, json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result.get("message", {}).get("content", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        error_text = await response.text()
                        logger.error(f"Ollama ì±„íŒ… ì˜¤ë¥˜: {response.status} - {error_text}")
                        return "ì±„íŒ… ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        
        except Exception as e:
            logger.error(f"ì±„íŒ… ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"ì±„íŒ… ì˜¤ë¥˜: {str(e)}"
    
    def get_model_info(self) -> dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "host": self.ollama_host,
            "port": self.ollama_port,
            "base_url": self.base_url
        }