"""
Router LLM Client for Conditional RAG
ì§ˆë¬¸ì´ RAGê°€ í•„ìš”í•œì§€ íŒë‹¨í•˜ëŠ” LLM í´ë¼ì´ì–¸íŠ¸
"""

import os
import logging
import httpx
from typing import Optional
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)

class RouterLLMClient:
    """RAG í•„ìš”ì„± íŒë‹¨ì„ ìœ„í•œ ë¼ìš°í„° LLM í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.host = os.environ["ROUTER_LLM_HOST"]
        self.port = os.environ["ROUTER_LLM_PORT"] 
        self.model = os.environ["ROUTER_LLM_MODEL"]
        self.base_url = f"http://{self.host}:{self.port}"
        
        logger.info(f"ğŸ¤– Router LLM ì´ˆê¸°í™”: {self.base_url} (ëª¨ë¸: {self.model})")
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        self._test_connection()
    
    def _test_connection(self):
        """Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            with httpx.Client(timeout=10.0) as client:
                # ì„œë²„ ìƒíƒœ í™•ì¸
                response = client.get(f"{self.base_url}/api/version")
                if response.status_code == 200:
                    logger.info("âœ… Router LLM ì„œë²„ ì—°ê²° ì„±ê³µ!")
                else:
                    logger.warning(f"âš ï¸ Router LLM ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                    
                # ëª¨ë¸ ì¡´ì¬ í™•ì¸
                response = client.get(f"{self.base_url}/api/tags")
                if response.status_code == 200:
                    models = response.json().get("models", [])
                    model_names = [m["name"] for m in models]
                    if self.model in model_names:
                        logger.info(f"âœ… Router ëª¨ë¸ '{self.model}' í™•ì¸ë¨")
                    else:
                        logger.warning(f"âš ï¸ Router ëª¨ë¸ '{self.model}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {model_names}")
                        
        except Exception as e:
            logger.error(f"âŒ Router LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def should_use_rag(self, query: str) -> bool:
        """
        ì§ˆë¬¸ì´ RAGê°€ í•„ìš”í•œì§€ íŒë‹¨
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            bool: RAG ì‚¬ìš© ì—¬ë¶€ (True: í•„ìš”, False: ë¶ˆí•„ìš”)
        """
        try:
            logger.info(f"ğŸ” Router íŒë‹¨ ìš”ì²­: {query[:50]}...")
            
            # ë¼ìš°í„° í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            router_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•´ì„œ UNCOMMON ì•ˆê²½ ì œí’ˆ ì •ë³´ê°€ í•„ìš”í•œì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: "{query}"

íŒë‹¨ ê¸°ì¤€:
- ì œí’ˆ ì •ë³´ í•„ìš”: ì•ˆê²½, ì„ ê¸€ë¼ìŠ¤, ì œí’ˆëª…, ê°€ê²©, ìƒ‰ìƒ, ì¬ì§ˆ, ì‚¬ì´ì¦ˆ, ë¸Œëœë“œ(UNCOMMON), êµ¬ë§¤, ì¶”ì²œ ë“±
- ì œí’ˆ ì •ë³´ ë¶ˆí•„ìš”: ì¼ë°˜ ì¸ì‚¬, ë‚ ì”¨, ë‰´ìŠ¤, ì¼ë°˜ ìƒì‹, ê°œì¸ì  ì§ˆë¬¸ ë“±

ë‹µë³€: ì œí’ˆì •ë³´í•„ìš” ë˜ëŠ” ì œí’ˆì •ë³´ë¶ˆí•„ìš” ì¤‘ í•˜ë‚˜ë§Œ ë‹µí•˜ì„¸ìš”."""

            # Ollama API í˜¸ì¶œ
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": router_prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.1,  # ì¼ê´€ëœ íŒë‹¨ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
                            "num_predict": 10,   # ì§§ì€ ë‹µë³€ë§Œ í•„ìš”
                            "stop": ["\n", ".", ","]  # ë‹µë³€ì„ ì§§ê²Œ ì œí•œ
                        }
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    answer = result.get("response", "").strip().lower()
                    
                    # ë‹µë³€ ë¶„ì„
                    needs_rag = "ì œí’ˆì •ë³´í•„ìš”" in answer and "ì œí’ˆì •ë³´ë¶ˆí•„ìš”" not in answer
                    
                    logger.info(f"ğŸ¯ Router íŒë‹¨ ê²°ê³¼: {'RAG í•„ìš”' if needs_rag else 'RAG ë¶ˆí•„ìš”'} (ë‹µë³€: {answer})")
                    return needs_rag
                    
                else:
                    logger.error(f"âŒ Router LLM API ì˜¤ë¥˜: {response.status_code}")
                    # ê¸°ë³¸ê°’ìœ¼ë¡œ RAG ì‚¬ìš© (ì•ˆì „í•œ ì„ íƒ)
                    return True
                    
        except Exception as e:
            logger.error(f"âŒ Router íŒë‹¨ ì‹¤íŒ¨: {str(e)}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ RAG ì‚¬ìš© (ì•ˆì „í•œ ì„ íƒ)
            return True
    
    async def generate_direct_response(self, query: str, temperature: float = 0.7) -> str:
        """
        RAG ì—†ì´ ì§ì ‘ ì‘ë‹µ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            temperature: ìƒì„± ì˜¨ë„
            
        Returns:
            str: ìƒì„±ëœ ì‘ë‹µ
        """
        try:
            logger.info(f"ğŸ’¬ Router LLM ì§ì ‘ ì‘ë‹µ ìƒì„±: {query[:50]}...")
            
            # ì§ì ‘ ì‘ë‹µìš© í”„ë¡¬í”„íŠ¸
            direct_prompt = f"""ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€:"""

            # Ollama API í˜¸ì¶œ
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": direct_prompt,
                        "stream": False,
                        "options": {
                            "temperature": temperature,
                            "num_predict": 200
                        }
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    answer = result.get("response", "").strip()
                    
                    logger.info(f"âœ… Router LLM ì§ì ‘ ì‘ë‹µ ìƒì„± ì™„ë£Œ ({len(answer)}ì)")
                    return answer
                    
                else:
                    logger.error(f"âŒ Router LLM ì§ì ‘ ì‘ë‹µ API ì˜¤ë¥˜: {response.status_code}")
                    return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    
        except Exception as e:
            logger.error(f"âŒ Router LLM ì§ì ‘ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê¸°ìˆ ì  ë¬¸ì œë¡œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    async def stream_direct_response(self, query: str, temperature: float = 0.7):
        """
        RAG ì—†ì´ ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            temperature: ìƒì„± ì˜¨ë„
            
        Yields:
            str: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬
        """
        try:
            logger.info(f"ğŸ“¡ Router LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘: {query[:50]}...")
            
            # ì§ì ‘ ì‘ë‹µìš© í”„ë¡¬í”„íŠ¸
            direct_prompt = f"""ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€:"""

            # Ollama ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ
            async with httpx.AsyncClient(timeout=60.0) as client:
                async with client.stream(
                    "POST",
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": direct_prompt,
                        "stream": True,
                        "options": {
                            "temperature": temperature,
                            "num_predict": 200
                        }
                    }
                ) as response:
                    
                    if response.status_code == 200:
                        async for line in response.aiter_lines():
                            if line:
                                try:
                                    import json
                                    chunk_data = json.loads(line)
                                    
                                    if "response" in chunk_data:
                                        chunk = chunk_data["response"]
                                        if chunk:
                                            yield chunk
                                            
                                    # ì™„ë£Œ ì²´í¬
                                    if chunk_data.get("done", False):
                                        logger.info("âœ… Router LLM ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
                                        break
                                        
                                except json.JSONDecodeError:
                                    continue
                                    
                    else:
                        logger.error(f"âŒ Router LLM ìŠ¤íŠ¸ë¦¬ë° API ì˜¤ë¥˜: {response.status_code}")
                        yield "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        
        except Exception as e:
            logger.error(f"âŒ Router LLM ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {str(e)}")
            yield "ì£„ì†¡í•©ë‹ˆë‹¤. ê¸°ìˆ ì  ë¬¸ì œë¡œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."