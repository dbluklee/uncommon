"""
ì„ë² ë”© ìƒì„± ëª¨ë“ˆ
BGE-M3 ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
"""

import os
import logging
from typing import List, Union
from sentence_transformers import SentenceTransformer
import torch
import numpy as np

logger = logging.getLogger(__name__)

class EmbeddingGenerator:
    """BGE-M3 ì„ë² ë”© ìƒì„±ê¸°"""
    
    def __init__(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        self.model_name = os.environ["EMBEDDING_MODEL"]
        self.use_cuda = os.environ["USE_CUDA"].lower() == "true"
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if self.use_cuda and torch.cuda.is_available():
            self.device = f"cuda:{os.environ['CUDA_DEVICE']}"
            logger.info(f"ğŸ® CUDA ì‚¬ìš©: {self.device}")
        else:
            self.device = "cpu"
            logger.info("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        
        # ëª¨ë¸ ë¡œë”©
        logger.info(f"ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {self.model_name}")
        try:
            self.model = SentenceTransformer(self.model_name, device=self.device)
            
            # ëª¨ë¸ í…ŒìŠ¤íŠ¸
            test_embedding = self.model.encode("test", convert_to_numpy=True)
            self.dimension = len(test_embedding)
            
            logger.info(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ì°¨ì›: {self.dimension})")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise
    
    def generate_embedding(self, text: Union[str, List[str]], normalize: bool = True) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸)
            normalize: ë²¡í„° ì •ê·œí™” ì—¬ë¶€
            
        Returns:
            numpy array í˜•íƒœì˜ ì„ë² ë”© ë²¡í„°
        """
        try:
            # ë‹¨ì¼ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if isinstance(text, str):
                text = [text]
            
            # ì„ë² ë”© ìƒì„±
            embeddings = self.model.encode(
                text,
                convert_to_numpy=True,
                normalize_embeddings=normalize,
                show_progress_bar=False
            )
            
            # ë‹¨ì¼ í…ìŠ¤íŠ¸ì¸ ê²½ìš° 1ì°¨ì› ë°°ì—´ ë°˜í™˜
            if len(text) == 1:
                return embeddings[0]
            
            return embeddings
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def generate_query_embedding(self, query: str) -> List[float]:
        """
        ê²€ìƒ‰ ì¿¼ë¦¬ìš© ì„ë² ë”© ìƒì„±
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ìì—´
            
        Returns:
            ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ì„ë² ë”© ë²¡í„°
        """
        try:
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬ (BGE-M3ëŠ” ì¿¼ë¦¬ì— íŠ¹ë³„í•œ í”„ë¦¬í”½ìŠ¤ í•„ìš” ì—†ìŒ)
            processed_query = query.strip()
            
            # ì„ë² ë”© ìƒì„±
            embedding = self.generate_embedding(processed_query, normalize=True)
            
            # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
            return embedding.tolist()
            
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def batch_generate_embeddings(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„±
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        all_embeddings = []
        
        try:
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                batch_embeddings = self.generate_embedding(batch, normalize=True)
                
                # numpy arrayë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                for embedding in batch_embeddings:
                    all_embeddings.append(embedding.tolist())
                
                logger.info(f"ë°°ì¹˜ {i//batch_size + 1}/{(len(texts) + batch_size - 1)//batch_size} ì²˜ë¦¬ ì™„ë£Œ")
            
            return all_embeddings
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def get_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self.dimension
    
    def get_model_info(self) -> dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "dimension": self.dimension,
            "device": self.device,
            "cuda_available": torch.cuda.is_available()
        }