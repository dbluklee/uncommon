# RAG API ì„œë¹„ìŠ¤

## ğŸ“‹ ê°œìš”
Retrieval-Augmented Generation (RAG) ê¸°ë°˜ì˜ ì§ˆì˜ì‘ë‹µ API ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ì œí’ˆ ì •ë³´ë¥¼ ì°¾ê³ , Ollama LLMì„ í†µí•´ ìì—°ì–´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì§€ì›í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥
- **ë²¡í„° ê²€ìƒ‰**: Milvus DBì—ì„œ ìœ ì‚¬ë„ ê¸°ë°˜ ì œí’ˆ ê²€ìƒ‰
- **RAG ì§ˆì˜ì‘ë‹µ**: ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLM ë‹µë³€ ìƒì„±
- **ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ**: Server-Sent Eventsë¡œ ì‹¤ì‹œê°„ ë‹µë³€ ì „ì†¡
- **ë©€í‹°ëª¨ë‹¬**: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ì—…ë¡œë“œ ì§€ì›
- **ê²€ìƒ‰ ì „ìš©**: LLM ì—†ì´ ìˆœìˆ˜ ë²¡í„° ê²€ìƒ‰ ê¸°ëŠ¥

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### í™˜ê²½ë³€ìˆ˜
```bash
RAG_API_PORT=8003                  # ì™¸ë¶€ ì ‘ê·¼ í¬íŠ¸
RAG_API_INTERNAL_PORT=8000        # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ í¬íŠ¸
OLLAMA_HOST=112.148.37.41         # ì™¸ë¶€ Ollama ì„œë²„
OLLAMA_PORT=1884                  # Ollama í¬íŠ¸
OLLAMA_MODEL=gemma3:27b-it-q4_K_M # LLM ëª¨ë¸ëª…
EMBEDDING_MODEL=BAAI/bge-m3       # ì„ë² ë”© ëª¨ë¸
USE_CUDA=true                     # CUDA ì‚¬ìš© ì—¬ë¶€
```

### ì‹¤í–‰ ëª…ë ¹
```bash
# ì„œë¹„ìŠ¤ ì‹œì‘
cd rag-api
source ../.env.global
docker compose up -d

# ë¡œì»¬ ê°œë°œ ì‹¤í–‰
pip install -r requirements.txt
python main.py
```

### ì ‘ì† ì •ë³´
- **API ì„œë²„**: `http://localhost:8003`
- **Swagger UI**: `http://localhost:8003/docs`

## ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸

### 1. ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
```http
GET /
```

**ì‘ë‹µ ì˜ˆì‹œ:**
```json
{
    "message": "RAG API Service is running",
    "ollama_host": "112.148.37.41:1884",
    "model": "gemma3:27b-it-q4_K_M",
    "embedding_model": "BAAI/bge-m3",
    "device": "cuda:0"
}
```

### 2. í—¬ìŠ¤ì²´í¬
```http
GET /health
```

**ì‘ë‹µ ì˜ˆì‹œ:**
```json
{
    "status": "healthy",
    "service": "rag-api",
    "ollama_connected": true,
    "milvus_connected": true,
    "model_loaded": true,
    "timestamp": "2024-01-10T10:30:00Z"
}
```

### 3. ë²¡í„° ê²€ìƒ‰ (LLM ì—†ìŒ)
```http
POST /search
Content-Type: application/json

{
    "query": "í‹°íƒ€ëŠ„ ì•ˆê²½í…Œ ì¶”ì²œí•´ì£¼ì„¸ìš”",
    "top_k": 5
}
```

**ì‘ë‹µ ì˜ˆì‹œ:**
```json
{
    "query": "í‹°íƒ€ëŠ„ ì•ˆê²½í…Œ ì¶”ì²œí•´ì£¼ì„¸ìš”",
    "results": [
        {
            "product_id": 123,
            "score": 0.95,
            "content": "UNCOMMON Titanium Frame Black í”„ë¦¬ë¯¸ì—„ í‹°íƒ€ëŠ„ í”„ë ˆì„...",
            "product_info": {
                "product_name": "UNCOMMON Titanium Frame",
                "color": "Black",
                "price": {"kr": "259,000ì›"},
                "material": {"kr": "í‹°íƒ€ëŠ„, ì•„ì„¸í…Œì´íŠ¸"}
            }
        }
    ],
    "processing_time": 0.15
}
```

### 4. RAG ì±„íŒ… (ìŠ¤íŠ¸ë¦¬ë°)
```http
POST /chat
Content-Type: application/json
Accept: text/event-stream

{
    "query": "ê°€ë²¼ìš´ ì•ˆê²½í…Œ ì¶”ì²œí•´ì£¼ì„¸ìš”",
    "stream": true,
    "top_k": 5
}
```

**ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì˜ˆì‹œ:**
```
data: {"type": "search_start", "message": "ê²€ìƒ‰ ì¤‘..."}

data: {"type": "search_results", "count": 3, "products": [...]}

data: {"type": "generation_start", "message": "ë‹µë³€ ìƒì„± ì¤‘..."}

data: {"type": "token", "content": "ì•ˆë…•í•˜ì„¸ìš”! "}

data: {"type": "token", "content": "ê°€ë²¼ìš´ ì•ˆê²½í…Œë¥¼ "}

data: {"type": "token", "content": "ì°¾ê³  ê³„ì‹œëŠ”êµ°ìš”. "}

data: {"type": "done", "message": "ë‹µë³€ ì™„ë£Œ"}
```

### 5. ë©€í‹°ëª¨ë‹¬ ì±„íŒ…
```http
POST /chat/multimodal
Content-Type: multipart/form-data

query: "ì´ ì•ˆê²½ê³¼ ë¹„ìŠ·í•œ ì œí’ˆ ì¶”ì²œí•´ì£¼ì„¸ìš”"
image: [ì´ë¯¸ì§€ íŒŒì¼]
stream: true
```

**ì‘ë‹µ**: `/chat`ì™€ ë™ì¼í•œ ìŠ¤íŠ¸ë¦¬ë° í˜•íƒœ

### 6. ì‹œìŠ¤í…œ í†µê³„
```http
GET /stats
```

**ì‘ë‹µ ì˜ˆì‹œ:**
```json
{
    "milvus_stats": {
        "collection": "uncommon_products",
        "entity_count": 150,
        "indexed_count": 150
    },
    "model_info": {
        "embedding_model": "BAAI/bge-m3",
        "llm_model": "gemma3:27b-it-q4_K_M",
        "device": "cuda:0"
    },
    "performance": {
        "avg_search_time": 0.12,
        "avg_generation_time": 2.3
    }
}
```

## ğŸ“Š ì…ë ¥/ì¶œë ¥ ë°ì´í„° í˜•ì‹

### ê²€ìƒ‰ ìš”ì²­
```json
{
    "query": "ê²€ì€ìƒ‰ í‹°íƒ€ëŠ„ ì•ˆê²½í…Œ",
    "top_k": 5,
    "score_threshold": 0.7
}
```

### ì±„íŒ… ìš”ì²­
```json
{
    "query": "ê°€ë²¼ìš´ ì•ˆê²½í…Œ ì¶”ì²œí•´ì£¼ì„¸ìš”",
    "stream": true,
    "top_k": 5,
    "context_window": 4000
}
```

### ê²€ìƒ‰ ê²°ê³¼ í˜•íƒœ
```python
search_results = {
    "query_embedding": [0.1, 0.2, ..., 0.9],  # 1024ì°¨ì›
    "retrieved_docs": [
        {
            "id": 1001,
            "product_id": 123,
            "score": 0.95,
            "text_content": "UNCOMMON Titanium Frame Black...",
            "metadata": {
                "product_name": "UNCOMMON Titanium Frame",
                "color": "Black",
                "material": {"kr": "í‹°íƒ€ëŠ„, ì•„ì„¸í…Œì´íŠ¸"}
            }
        }
    ]
}
```

### LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
```python
prompt_template = """
ë‹¤ìŒì€ UNCOMMON ì•„ì´ì›¢ì–´ ì œí’ˆì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤:

{retrieved_context}

ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

ìœ„ ì œí’ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì œí’ˆì˜ íŠ¹ì§•, ê°€ê²©, ì¬ì§ˆ ë“±ì„ í¬í•¨í•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ì¶”ì²œí•´ì£¼ì„¸ìš”.
"""
```

## ğŸ”„ í†µì‹  ë°©ì‹

### HTTP REST API + SSE
```python
# ì¼ë°˜ JSON ì‘ë‹µ
@app.post("/search")
async def search_products(request: SearchRequest):
    return JSONResponse(content=results)

# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ  
@app.post("/chat")
async def chat_stream(request: ChatRequest):
    return StreamingResponse(
        generate_streaming_response(request),
        media_type="text/event-stream"
    )
```

### Ollama LLM í†µì‹ 
```python
import requests

def stream_ollama_response(prompt: str):
    response = requests.post(
        f"http://{OLLAMA_HOST}:{OLLAMA_PORT}/api/generate",
        json={
            "model": OLLAMA_MODEL,
            "prompt": prompt,
            "stream": True
        },
        stream=True
    )
    
    for line in response.iter_lines():
        if line:
            yield json.loads(line)["response"]
```

### Milvus ë²¡í„° ê²€ìƒ‰
```python
from pymilvus import Collection

collection = Collection("uncommon_products")
search_params = {
    "metric_type": "COSINE",
    "params": {"nprobe": 10}
}

results = collection.search(
    data=[query_embedding],
    anns_field="embedding", 
    param=search_params,
    limit=top_k,
    output_fields=["product_id", "text_content"]
)
```

## ğŸ”— ì˜ì¡´ì„±

### í•„ìˆ˜ ì˜ì¡´ì„±
```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
sentence-transformers==2.2.2
pymilvus==2.3.3
requests==2.31.0
sqlalchemy==2.0.23
asyncpg==0.29.0
python-multipart==0.0.6
torch==2.1.0
Pillow==10.1.0
```

### ì™¸ë¶€ ì„œë¹„ìŠ¤ ì˜ì¡´ì„±
- **Ollama LLM ì„œë²„**: `112.148.37.41:1884`
  - ëª¨ë¸: `gemma3:27b-it-q4_K_M`
  - GPU ê°€ì† ì™¸ë¶€ ì„œë²„
- **Milvus ë²¡í„°DB**: ì œí’ˆ ë²¡í„° ê²€ìƒ‰
- **PostgreSQL DB**: ì œí’ˆ ìƒì„¸ ì •ë³´ ì¡°íšŒ

### ì—°ê´€ ì„œë¹„ìŠ¤
- **Web App**: ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ì œê³µ
- **Indexing Service**: ë²¡í„° ë°ì´í„° ìƒì„±

## ğŸ§  RAG íŒŒì´í”„ë¼ì¸

### 1. ì¿¼ë¦¬ ì„ë² ë”©
```python
async def embed_query(query: str) -> List[float]:
    # BGE-M3 ëª¨ë¸ë¡œ ì§ˆë¬¸ ë²¡í„°í™”
    embedding = embedding_model.encode([query], normalize_embeddings=True)
    return embedding[0].tolist()
```

### 2. ë²¡í„° ê²€ìƒ‰
```python
async def vector_search(query_embedding: List[float], top_k: int = 5):
    results = collection.search(
        data=[query_embedding],
        anns_field="embedding",
        param={"metric_type": "COSINE", "params": {"nprobe": 10}},
        limit=top_k,
        output_fields=["product_id", "text_content"]
    )
    return results[0]
```

### 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
```python
def build_context(search_results) -> str:
    context = ""
    for result in search_results:
        product_info = get_product_details(result.entity.get("product_id"))
        context += f"ì œí’ˆ: {product_info['product_name']}\n"
        context += f"ì„¤ëª…: {result.entity.get('text_content')}\n"
        context += f"ê°€ê²©: {product_info['price']['kr']}\n\n"
    return context
```

### 4. LLM ìƒì„±
```python
async def generate_response(prompt: str):
    async for chunk in ollama_stream_generate(prompt):
        yield {
            "type": "token",
            "content": chunk,
            "timestamp": datetime.now().isoformat()
        }
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### ìºì‹± ì „ëµ
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_embedding(text: str) -> List[float]:
    return embedding_model.encode([text])[0].tolist()

# ê²€ìƒ‰ ê²°ê³¼ ìºì‹± (Redis ê¶Œì¥, í˜„ì¬ëŠ” ë©”ëª¨ë¦¬)
search_cache = {}
```

### ë°°ì¹˜ ì²˜ë¦¬
```python
# ì—¬ëŸ¬ ì¿¼ë¦¬ ë™ì‹œ ì²˜ë¦¬
async def batch_search(queries: List[str]):
    embeddings = embedding_model.encode(queries, batch_size=32)
    results = []
    for embedding in embeddings:
        result = await vector_search(embedding.tolist())
        results.append(result)
    return results
```

### ì—°ê²° í’€ë§
```python
# Milvus ì—°ê²° í’€
from pymilvus import connections
connections.connect(
    alias="default",
    host=MILVUS_HOST,
    port=MILVUS_INTERNAL_PORT,
    pool_size=10
)
```

## ğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­

### ì‘ë‹µ ì‹œê°„
```python
performance_metrics = {
    "embedding_time": "50-100ms",
    "vector_search_time": "20-50ms", 
    "llm_first_token": "500-1000ms",
    "llm_streaming": "50-100 tokens/sec",
    "total_response": "2-5 seconds"
}
```

### ì •í™•ë„ ì¸¡ì •
```python
def calculate_search_relevance(query, results):
    relevance_scores = []
    for result in results:
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì ìˆ˜
        relevance_scores.append(result.distance)
    return {
        "avg_relevance": np.mean(relevance_scores),
        "max_relevance": max(relevance_scores),
        "results_count": len(results)
    }
```

## ğŸ” ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### ìƒì„¸ ë¡œê¹…
```python
import logging

logger = logging.getLogger(__name__)

async def log_chat_session(query, results, response_time):
    logger.info(f"Chat Query: {query}")
    logger.info(f"Search Results: {len(results)} products found") 
    logger.info(f"Response Time: {response_time:.2f}s")
    logger.info(f"Top Relevance Score: {results[0].distance if results else 'N/A'}")
```

### ì—ëŸ¬ ì²˜ë¦¬
```python
try:
    search_results = await vector_search(query_embedding)
    llm_response = await generate_llm_response(context)
except Exception as e:
    logger.error(f"RAG Pipeline Error: {e}")
    yield {"type": "error", "message": "ì„œë¹„ìŠ¤ ì¼ì‹œ ì¥ì• . ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}
```

## âš ï¸ ì£¼ì˜ì‚¬í•­
- **ì™¸ë¶€ ì˜ì¡´ì„±**: Ollama ì„œë²„ ë‹¤ìš´ ì‹œ RAG ê¸°ëŠ¥ ë¶ˆê°€ (ê²€ìƒ‰ì€ ê°€ëŠ¥)
- **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ì„ë² ë”© ëª¨ë¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì£¼ì˜
- **ìŠ¤íŠ¸ë¦¬ë° ì—°ê²°**: í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€ ê°ì§€ ë° ì²˜ë¦¬
- **ë™ì‹œ ìš”ì²­**: ë†’ì€ ë™ì‹œì„±ì—ì„œ Ollama ì„œë²„ ë¶€í•˜ ê´€ë¦¬
- **í† í° ì œí•œ**: LLM ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì´ˆê³¼ ë°©ì§€ (4000ì ì œí•œ)