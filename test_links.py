#!/usr/bin/env python3
"""
ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ì œí’ˆ ë§í¬ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
"""
import requests
from bs4 import BeautifulSoup
import random
import time

def get_product_links_like_scraper(base_url):
    """ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ì œí’ˆ ë§í¬ ì¶”ì¶œ"""
    
    # ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ User-Agent ì„¤ì •
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'
    ]
    
    # ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ í—¤ë” ì„¤ì •
    session = requests.Session()
    session.headers.update({
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'same-origin',
        'Cache-Control': 'max-age=0'
    })
    
    all_product_links = []
    page_num = 1
    
    print(f"ğŸ” ìŠ¤í¬ë˜í¼ ë¡œì§ìœ¼ë¡œ ì œí’ˆ ë§í¬ ì¶”ì¶œ ì‹œì‘: {base_url}")
    
    try:
        # í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ëª¨ë“  ì œí’ˆ ë§í¬ ìˆ˜ì§‘ (ìŠ¤í¬ë˜í¼ì™€ ë™ì¼)
        while True:
            # í˜ì´ì§€ URL êµ¬ì„±
            page_url = f"{base_url}?page={page_num}"
            print(f"\nğŸ“„ í˜ì´ì§€ {page_num} ìŠ¤í¬ë˜í•‘ ì¤‘: {page_url}")
            
            response = session.get(page_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # *** ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ ë¡œì§: prdImg divë“¤ì„ ì°¾ì•„ì„œ ì œí’ˆ ë§í¬ ì¶”ì¶œ ***
            prd_imgs = soup.find_all('div', class_='prdImg')
            
            # prdImgê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ìŠ¤í¬ë˜í•‘ ì¢…ë£Œ
            if not prd_imgs:
                print(f"âŒ í˜ì´ì§€ {page_num}ì—ì„œ prdImg divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ìŠ¤í¬ë˜í•‘ ì¢…ë£Œ.")
                break
            
            page_links = []
            for prd_img in prd_imgs:
                # prdImg div ì•ˆì˜ a íƒœê·¸ ì°¾ê¸°
                link = prd_img.find('a', href=True)
                if link:
                    href = link['href']
                    # ìƒëŒ€ê²½ë¡œë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜ (ìŠ¤í¬ë˜í¼ì™€ ë™ì¼)
                    if href.startswith('/'):
                        full_url = f"https://ucmeyewear.earth{href}"
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        full_url = f"https://ucmeyewear.earth/{href}"
                    
                    # ì¤‘ë³µ ì œê±° (ìŠ¤í¬ë˜í¼ì™€ ë™ì¼)
                    if full_url not in all_product_links:
                        page_links.append(full_url)
                        all_product_links.append(full_url)
            
            print(f"âœ… í˜ì´ì§€ {page_num}: {len(page_links)}ê°œì˜ ìƒˆë¡œìš´ ì œí’ˆ ë§í¬ ë°œê²¬")
            
            # ì´ í˜ì´ì§€ì˜ ë§í¬ë“¤ ì¶œë ¥
            for i, link in enumerate(page_links, 1):
                link_num = len(all_product_links) - len(page_links) + i
                print(f"  {link_num:2d}. {link}")
            
            # ë‹¤ìŒ í˜ì´ì§€ë¡œ
            page_num += 1
            
            # í˜ì´ì§€ ê°„ ëŒ€ê¸° (ìŠ¤í¬ë˜í¼ì™€ ë™ì¼)
            delay = random.uniform(2, 5)
            print(f"â±ï¸  ë‹¤ìŒ í˜ì´ì§€ê¹Œì§€ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
            time.sleep(delay)
        
        print(f"\nğŸ“Š ì´ {len(all_product_links)}ê°œì˜ ê³ ìœ í•œ ì œí’ˆ ë§í¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        return all_product_links
        
    except requests.RequestException as e:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

if __name__ == "__main__":
    base_url = "https://ucmeyewear.earth/category/all/87/"
    print("ğŸš€ ìŠ¤í¬ë˜í¼ì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ì œí’ˆ ë§í¬ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    links = get_product_links_like_scraper(base_url)
    
    if links:
        print(f"\nâœ¨ ì„±ê³µì ìœ¼ë¡œ {len(links)}ê°œ ì œí’ˆ ë§í¬ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤!")
        print("\nğŸ“‹ ì¶”ì¶œëœ ëª¨ë“  ì œí’ˆ ë§í¬:")
        for i, link in enumerate(links, 1):
            print(f"{i:2d}. {link}")
    else:
        print("\nâŒ ì œí’ˆ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")