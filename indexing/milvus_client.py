"""
Milvus ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸ - UNCOMMON í”„ë¡œì íŠ¸ ì „ìš©
langchain-milvusë¥¼ ì‚¬ìš©í•œ ê³ ë„í™”ëœ ë²¡í„° ìŠ¤í† ì–´
"""

from typing import List, Dict, Any, Optional
from langchain_milvus import Milvus
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_core.documents import Document
from langchain.vectorstores.base import VectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection
import os
import logging
from dotenv import load_dotenv

# Load environment variables
load_dotenv('../.env.global')
load_dotenv()

logger = logging.getLogger(__name__)

class ProductMilvusVectorStore(VectorStore):
    """ì œí’ˆ ë°ì´í„° ì „ìš© Milvus ë²¡í„° ìŠ¤í† ì–´"""
    
    def __init__(self, 
                 collection_name: str = "uncommon_products",
                 embedding_model: HuggingFaceEmbeddings = None,
                 metric_type: str = 'IP',
                 index_type: str = 'HNSW',
                 milvus_host: str = None,
                 milvus_port: str = None,
                 always_new: bool = False):
        """
        Milvus Vector Store for UNCOMMON Products
        
        Args:
            collection_name: Milvus ì»¬ë ‰ì…˜ ì´ë¦„
            embedding_model: ì„ë² ë”© ìƒì„±ìš© ëª¨ë¸
            milvus_host: Milvus ì„œë²„ í˜¸ìŠ¤íŠ¸
            milvus_port: Milvus ì„œë²„ í¬íŠ¸
            always_new: ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„± ì—¬ë¶€
        """
        self.collection_name = collection_name
        self.embedding_model = embedding_model
        self.embedding_dim = 1024  # BAAI/bge-m3 ëª¨ë¸ì˜ ì„ë² ë”© ì°¨ì›
        self.always_new = always_new
        self.metric_type = metric_type
        self.index_type = index_type
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ Milvus ì ‘ì† ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì»¨í…Œì´ë„ˆ ê°„ í†µì‹ ìš© ë‚´ë¶€ í¬íŠ¸ ì‚¬ìš©)
        self.milvus_host = milvus_host or os.environ['MILVUS_HOST']
        self.milvus_port = milvus_port or os.environ['MILVUS_INTERNAL_PORT']
        
        # Milvus ì—°ê²°
        print(f"\nğŸ”— Milvus ì—°ê²° ì‹œë„: {self.milvus_host}:{self.milvus_port}")
        connections.connect("default", host=self.milvus_host, port=self.milvus_port)

        # ì„œë²„ ë²„ì „ ì •ë³´ë¥¼ ìš”ì²­í•˜ì—¬ ì‹¤ì œ í†µì‹  í™•ì¸
        server_version = utility.get_server_version()
        print(f"âœ… Milvus ì—°ê²° ì„±ê³µ! (ì„œë²„ ë²„ì „: {server_version})")
        
        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ë¡œë“œ
        self._setup_collection()

    def _setup_collection(self):
        """Milvus ì»¬ë ‰ì…˜ ì„¤ì •"""
        print(f"\nğŸ“‹ ì»¬ë ‰ì…˜ ì„¤ì •: {self.collection_name}")
        
        # ìŠ¤í‚¤ë§ˆ ì •ì˜
        fields = [
            # id í•„ë“œ (ìë™ ID ìƒì„±)
            FieldSchema(name="pk", dtype=DataType.VARCHAR, is_primary=True, auto_id=True, max_length=100),
            # ë²¡í„°ë¥¼ ì €ì¥í•  í•„ë“œ
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim),
            # ì œí’ˆ ì •ë³´ í•„ë“œë“¤
            FieldSchema(name="product_id", dtype=DataType.INT64),
            FieldSchema(name="product_name", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="chunk_type", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=200),
            # ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  í•„ë“œ
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535)
        ]
        
        schema = CollectionSchema(fields, f"'{self.collection_name}' UNCOMMON Product Documents")
        
        if self.always_new:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ
            if utility.has_collection(self.collection_name):
                print(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{self.collection_name}' ì‚­ì œ")
                utility.drop_collection(self.collection_name)

        # ì»¬ë ‰ì…˜ ìƒì„±
        try:
            self.collection = Collection(self.collection_name, schema)
            print(f"âœ… ìƒˆ ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„±")
        except Exception:
            self.collection = Collection(self.collection_name)
            print(f"âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ '{self.collection_name}' ë¡œë“œ")
        
        # ì¸ë±ìŠ¤ ìƒì„±
        self._create_index()

    def _create_index(self):
        """ë²¡í„° í•„ë“œì— ì¸ë±ìŠ¤ ìƒì„±"""
        if self.index_type == 'HNSW':
            params = {"M": 8, "efConstruction": 64}
        elif self.index_type in ["IVF_FLAT", "IVF_SQ8", "IVF_PQ"]:
            params = {"nlist": 128}
        else:
            params = {}

        index_params = {
            "metric_type": self.metric_type,  
            "index_type": self.index_type,
            "params": params
        }
        
        try:
            print(f"ğŸ”§ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            self.collection.create_index("vector", index_params)
            print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ (metric_type: {self.metric_type}, index_type: {self.index_type})")
        except Exception as e:
            print(f"âš ï¸ ì¸ë±ìŠ¤ ìƒì„± ì˜¤ë¥˜ (ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìŒ): {e}")

    def add_texts(self, texts: List[str], metadatas: Optional[List[dict]] = None, **kwargs) -> List[str]:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€ (ë°°ì¹˜ ì²˜ë¦¬)
        """
        if metadatas is None:
            metadatas = [{}] * len(texts)
        
        print(f"\nğŸ“¤ {len(texts)}ê°œ ë¬¸ì„œë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì • (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
        BATCH_SIZE = 16  # í•œ ë²ˆì— 16ê°œì”© ì²˜ë¦¬
        
        # ì „ì²´ ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        all_vectors = []
        for i in range(0, len(texts), BATCH_SIZE):
            batch_texts = texts[i:i+BATCH_SIZE]
            print(f"   ë°°ì¹˜ {i//BATCH_SIZE + 1}/{(len(texts)-1)//BATCH_SIZE + 1}: {len(batch_texts)}ê°œ ë¬¸ì„œ ì„ë² ë”© ì¤‘...")
            
            try:
                # ë°°ì¹˜ë³„ ì„ë² ë”© ìƒì„±
                batch_vectors = self.embedding_model.embed_documents(batch_texts)
                all_vectors.extend(batch_vectors)
                print(f"   âœ… ë°°ì¹˜ ì™„ë£Œ ({len(batch_vectors)}ê°œ ë²¡í„° ìƒì„±)")
                
            except RuntimeError as e:
                if "CUDA" in str(e):
                    print(f"   âŒ CUDA ë©”ëª¨ë¦¬ ì˜¤ë¥˜ ë°œìƒ, ë” ì‘ì€ ë°°ì¹˜ë¡œ ì¬ì‹œë„...")
                    # ë” ì‘ì€ ë°°ì¹˜ë¡œ ì¬ì‹œë„
                    for j in range(i, min(i+BATCH_SIZE, len(texts))):
                        single_vector = self.embedding_model.embed_documents([texts[j]])
                        all_vectors.extend(single_vector)
                        print(f"     ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬: {j+1}/{len(texts)}")
                else:
                    raise e
        
        print(f"âœ… ì „ì²´ {len(all_vectors)}ê°œ ë²¡í„° ìƒì„± ì™„ë£Œ")
        
        # ë°ì´í„° ì¤€ë¹„
        product_ids = []
        product_names = []
        chunk_types = []
        sources = []
        contents = []
        
        for i, text in enumerate(texts):
            metadata = metadatas[i]
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            product_id = metadata.get('product_id', 0)
            product_name = metadata.get('product_name', '')
            chunk_type = metadata.get('chunk_type', 'unknown')
            source = metadata.get('source', '')

            # ë°ì´í„° ì¶”ê°€
            product_ids.append(product_id)
            product_names.append(product_name)
            chunk_types.append(chunk_type)
            sources.append(source)
            contents.append(text)
        
        # Milvusì— ì‚½ì…í•  ë°ì´í„° êµ¬ì„±
        data = [
            all_vectors,  # ë°°ì¹˜ë¡œ ìƒì„±ëœ ì „ì²´ ë²¡í„°
            product_ids,
            product_names,
            chunk_types,
            sources,
            contents
        ]

        # ì»¬ë ‰ì…˜ ë¡œë“œ (ê²€ìƒ‰ì„ ìœ„í•´ í•„ìš”)
        self.collection.load()
        
        # ë°ì´í„° ì‚½ì…
        mr = self.collection.insert(data)
        print(f"âœ… {len(texts)}ê°œ ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì‚½ì…ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„° í”ŒëŸ¬ì‹œ (ì˜êµ¬ ì €ì¥)
        self.collection.flush()
        print("âœ… ë°ì´í„°ê°€ ì˜êµ¬ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return mr.primary_keys  

    def add_documents(self, documents: List[Document], **kwargs) -> List[str]:
        """Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€"""
        texts = [doc.page_content for doc in documents]
        metadatas = [doc.metadata for doc in documents]
        return self.add_texts(texts, metadatas, **kwargs)

    def similarity_search(self, query: str, k: int = 4, **kwargs) -> List[Document]:
        """ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰ (LangChain ì¸í„°í˜ì´ìŠ¤)"""
        # ë¨¼ì € ì»¬ë ‰ì…˜ì˜ ì´ ë¬¸ì„œ ìˆ˜ í™•ì¸
        self.collection.load()
        total_docs = self.collection.num_entities
        print(f"\nğŸ“Š ì»¬ë ‰ì…˜ ì´ ë¬¸ì„œ ìˆ˜: {total_docs}")
        print(f"ğŸ“Š ìš”ì²­ëœ k ê°’: {k}")
        
        # ì‹¤ì œ k ê°’ ì¡°ì • (ì´ ë¬¸ì„œ ìˆ˜ë³´ë‹¤ í´ ìˆ˜ ì—†ìŒ)
        actual_k = min(k, total_docs)
        print(f"ğŸ“Š ì‹¤ì œ ê²€ìƒ‰í•  k ê°’: {actual_k}")
        
        if total_docs == 0:
            print("âš ï¸ ì»¬ë ‰ì…˜ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        print(f"\nğŸ” ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±: '{query}'")
        query_vector = self.embedding_model.embed_query(query)
        print(f"ğŸ“ ì¿¼ë¦¬ ë²¡í„° ì°¨ì›: {len(query_vector)}")

        # ì¸ë±ìŠ¤ ì •ë³´ í™•ì¸
        try:
            vector_index = self.collection.indexes[0]
            index_type = vector_index.params.get("index_type")
            metric_type = vector_index.params.get("metric_type")

            if index_type == 'HNSW':
                params = {"ef": 64}
            elif index_type in ["IVF_FLAT", "IVF_SQ8", "IVF_PQ"]:
                params = {"nprobe": 10}
            else:
                params = {}
        except:
            # ê¸°ë³¸ê°’ ì‚¬ìš©
            metric_type = self.metric_type
            index_type = self.index_type
            params = {}

        # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
        print(f"\nğŸ”§ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°:")
        print(f"   - metric_type: {metric_type}")
        print(f"   - index_type: {index_type}")
        print(f"   - params: {params}")
        print(f"   - limit: {actual_k}")
        
        search_params = {"metric_type": metric_type, "params": params}
        
        # ê²€ìƒ‰ ì‹¤í–‰
        print(f"\nğŸ” ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
        try:
            results = self.collection.search(
                data=[query_vector],
                anns_field="vector",
                param=search_params,
                limit=actual_k,
                output_fields=["product_id", "product_name", "chunk_type", "source", "content"]
            )
            
            print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ!")
            print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜: {len(results[0]) if results else 0}")
            
            # ê° ê²°ê³¼ì˜ ìƒì„¸ ì •ë³´ ì¶œë ¥
            if results and len(results[0]) > 0:
                for i, hit in enumerate(results[0]):
                    print(f"   ê²°ê³¼ {i+1}: score={hit.score:.4f}, product_id={hit.entity.get('product_id')}")
                    print(f"          ì œí’ˆëª…: {hit.entity.get('product_name', 'N/A')}")
                    print(f"          ì²­í¬íƒ€ì…: {hit.entity.get('chunk_type', 'N/A')}")
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
        
        # LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        print(f"\nğŸ”„ LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        docs = []
        for hits in results:
            for hit in hits:
                doc = Document(
                    page_content=hit.entity.get("content"),
                    metadata={
                        "product_id": hit.entity.get("product_id"),
                        "product_name": hit.entity.get("product_name"),
                        "chunk_type": hit.entity.get("chunk_type"),
                        "source": hit.entity.get("source"),
                        "score": hit.score,
                        "id": hit.id
                    }
                )
                docs.append(doc)
        
        print(f"âœ… {len(docs)}ê°œ ë¬¸ì„œë¥¼ LangChain Documentë¡œ ë³€í™˜ ì™„ë£Œ")
        return docs
    
    def similarity_search_with_score(self, query: str, k: int = 4, **kwargs) -> List[tuple]:
        """ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰"""
        docs = self.similarity_search(query, k, **kwargs)
        return [(doc, doc.metadata.get('score', 0.0)) for doc in docs]

    @classmethod
    def from_texts(cls, texts: List[str], embedding_model: HuggingFaceEmbeddings, metadatas: Optional[List[dict]] = None, **kwargs):
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        vector_store = cls(embedding_model=embedding_model, **kwargs)
        vector_store.add_texts(texts, metadatas)
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
        return vector_store

    @classmethod
    def from_documents(cls, documents: List[Document], embedding_model: HuggingFaceEmbeddings, **kwargs):
        """Document ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        vector_store = cls(embedding_model=embedding_model, **kwargs)
        vector_store.add_documents(documents)
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
        return vector_store