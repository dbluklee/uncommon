"""
BGE-M3 ì„ë² ë”© ìƒì„± ëª¨ë“ˆ - ì•ˆì •í™” ë²„ì „
sentence-transformersë¥¼ ì§ì ‘ ì‚¬ìš©í•œ BGE-M3 ëª¨ë¸
"""

import torch
from sentence_transformers import SentenceTransformer
import os
import subprocess
import sys
from pathlib import Path
from typing import List, Dict, Any
import logging
import numpy as np

logger = logging.getLogger(__name__)

def download_model_automatically(model_path: str, model_name: str = "BAAI/bge-m3") -> bool:
    """ìë™ìœ¼ë¡œ ì„ë² ë”© ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        print(f"ğŸš€ ìë™ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        print(f"   ëª¨ë¸: {model_name}")
        print(f"   ì €ì¥ ìœ„ì¹˜: {model_path}")
        
        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(model_path).mkdir(parents=True, exist_ok=True)
        
        # HuggingFace Hub ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ
        try:
            from huggingface_hub import snapshot_download
            print("ğŸ“¥ HuggingFace Hubì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            print("   âš ï¸ ëŒ€ìš©ëŸ‰ íŒŒì¼ì…ë‹ˆë‹¤. ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            downloaded_path = snapshot_download(
                repo_id=model_name,
                local_dir=model_path,
                local_dir_use_symlinks=False,
                resume_download=True
            )
            
            print(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {downloaded_path}")
            return True
            
        except ImportError:
            print("âŒ huggingface_hubê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   pip install huggingface_hubë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return False
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
            
    except Exception as e:
        print(f"âŒ ìë™ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def verify_model_files(model_path: str) -> bool:
    """ëª¨ë¸ íŒŒì¼ì´ ì™„ì „íˆ ë‹¤ìš´ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    required_files = ["config.json"]
    optional_files = ["pytorch_model.bin", "model.safetensors"]
    
    # í•„ìˆ˜ íŒŒì¼ í™•ì¸
    for file in required_files:
        if not Path(model_path + "/" + file).exists():
            return False
    
    # ëª¨ë¸ íŒŒì¼ ì¤‘ í•˜ë‚˜ëŠ” ìˆì–´ì•¼ í•¨
    has_model_file = any(Path(model_path + "/" + file).exists() for file in optional_files)
    if not has_model_file:
        return False
    
    return True

def get_bge_m3_model():
    """
    BGE-M3 ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    ë¡œì»¬ ëª¨ë¸ ìš°ì„ , ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
    USE_CUDA í™˜ê²½ë³€ìˆ˜ì— ë”°ë¼ CPU/GPUë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    """
    
    # USE_CUDA í™˜ê²½ë³€ìˆ˜ í™•ì¸
    use_cuda = os.environ['USE_CUDA'].lower() == 'true'
    
    # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    local_model_path = "/app/models/bge-m3"
    huggingface_model_name = 'BAAI/bge-m3'
    
    # ë¡œì»¬ ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    local_model_exists = verify_model_files(local_model_path)
    
    # ëª¨ë¸ ì„ íƒ ë¡œì§
    if local_model_exists:
        print(f"ğŸ¯ ë¡œì»¬ ëª¨ë¸ ë°œê²¬: {local_model_path}")
        model_name = local_model_path
        
        # ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ì •ë³´ ì¶œë ¥
        try:
            model_files = list(Path(local_model_path).glob("*"))
            total_size = sum(f.stat().st_size for f in model_files if f.is_file()) / (1024*1024*1024)
            print(f"ğŸ“ ë¡œì»¬ ëª¨ë¸ í¬ê¸°: {total_size:.1f}GB")
            print(f"ğŸ“‹ ëª¨ë¸ íŒŒì¼ ìˆ˜: {len([f for f in model_files if f.is_file()])}ê°œ")
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ì •ë³´ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            
    else:
        print(f"âš ï¸ ë¡œì»¬ ëª¨ë¸ ì—†ìŒ: {local_model_path}")
        print(f"ğŸŒ ëª¨ë¸ ì†ŒìŠ¤: {huggingface_model_name}")
        model_name = huggingface_model_name
    
    # USE_CUDAê°€ falseë©´ CUDA_VISIBLE_DEVICESë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •í•˜ì—¬ GPU ë¹„í™œì„±í™”
    if not use_cuda:
        os.environ['CUDA_VISIBLE_DEVICES'] = ''
        print("ğŸ”§ USE_CUDA=false - GPU ë¹„í™œì„±í™”, CPU ëª¨ë“œë¡œ ì „í™˜")
        device = 'cpu'
    elif torch.cuda.is_available():
        device = 'cuda'
    else:
        print("âš ï¸ CUDA ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ, CPU ëª¨ë“œë¡œ ì „í™˜")
        device = 'cpu'
    
    print(f"ğŸ”§ ì„ë² ë”© ëª¨ë¸ ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ“ ëª¨ë¸ ì†ŒìŠ¤: {'ë¡œì»¬ íŒŒì¼' if model_name == local_model_path else 'HuggingFace Hub'}")
    
    model_kwargs = {'device': device}
    encode_kwargs = {'normalize_embeddings': True}
    
    try:
        print(f"â³ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # sentence-transformersë¥¼ ì§ì ‘ ì‚¬ìš©
        model = SentenceTransformer(model_name, device=device)
        
        # ë¡œë”© ì„±ê³µ í›„ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        print(f"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_embedding = model.encode("test", convert_to_numpy=True)
        embedding_dim = len(test_embedding)
        
        print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        print(f"   ğŸ“ ì„ë² ë”© ì°¨ì›: {embedding_dim}")
        print(f"   ğŸ¯ ë””ë°”ì´ìŠ¤: {device}")
        print(f"   ğŸ“ ëª¨ë¸ ê²½ë¡œ: {model_name}")
        
        # langchain í˜¸í™˜ì„ ìœ„í•œ ë˜í¼ í´ë˜ìŠ¤
        class SentenceTransformerWrapper:
            def __init__(self, model):
                self.model = model
                
            def embed_query(self, text: str) -> List[float]:
                """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©"""
                embedding = self.model.encode(text, convert_to_numpy=True, normalize_embeddings=True)
                return embedding.tolist()
                
            def embed_documents(self, texts: List[str]) -> List[List[float]]:
                """ì—¬ëŸ¬ ë¬¸ì„œ ì„ë² ë”©"""
                embeddings = self.model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)
                return embeddings.tolist()
        
        return SentenceTransformerWrapper(model)
        
    except Exception as e:
        error_msg = f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}\n"
        
        if model_name == local_model_path:
            error_msg += "í•´ê²°ë°©ë²•:\n"
            error_msg += "1. ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ì†ìƒ í™•ì¸\n"
            error_msg += "2. ëª¨ë¸ ì¬ë‹¤ìš´ë¡œë“œ\n"
            error_msg += "3. USE_CUDA=falseë¡œ ì„¤ì •í•˜ì—¬ CPU ëª¨ë“œ ì‹œë„\n"
            error_msg += "4. HuggingFace Hub ëª¨ë“œë¡œ ì „í™˜ (ë¡œì»¬ ëª¨ë¸ ì‚­ì œ)"
        else:
            error_msg += "í•´ê²°ë°©ë²•:\n"
            error_msg += "1. ì¸í„°ë„· ì—°ê²° í™•ì¸ (HuggingFace ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)\n"
            error_msg += "2. ë¡œì»¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n"
            error_msg += "3. ë””ìŠ¤í¬ ê³µê°„ í™•ì¸\n"
            error_msg += "4. USE_CUDA=falseë¡œ ì„¤ì •í•˜ì—¬ CPU ëª¨ë“œ ì‹œë„"
        
        raise RuntimeError(error_msg)