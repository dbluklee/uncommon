"""
UNCOMMON ì œí’ˆ ì¸ë±ì‹± ì„œë¹„ìŠ¤ - ê°œì„ ëœ ë²„ì „
BGE-M3 + Milvusë¥¼ ì‚¬ìš©í•œ ì œí’ˆ ë°ì´í„° ë²¡í„°í™”
"""

import os
import json
import logging
from datetime import datetime
from typing import List, Dict, Any
from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends, Header
from pydantic import BaseModel
from sqlalchemy.orm import Session
from langchain_core.documents import Document
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from database import get_db, init_db, Product, ProductImage
from text_chunker import ProductTextChunker, ProductChunk
from embedding_generator import get_bge_m3_model
from milvus_client import ProductMilvusVectorStore

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv('../.env.global')
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•±
app = FastAPI(
    title="UNCOMMON Indexing Service",
    description="ì œí’ˆ ë°ì´í„° ë²¡í„°í™” ë° Milvus ì¸ë±ì‹± ì„œë¹„ìŠ¤",
    version="1.0.0"
)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
embedding_model = None
vector_store = None
chunker = None

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class IndexRequest(BaseModel):
    force_reindex: bool = False
    product_ids: List[int] = []  # íŠ¹ì • ì œí’ˆë§Œ ì¸ë±ì‹±

class IndexResponse(BaseModel):
    message: str
    total_products: int
    indexed_count: int
    errors: List[str] = []

class StatsResponse(BaseModel):
    total_products: int
    indexed_products: int
    pending_products: int
    milvus_documents: int

# ì¸ì¦
def verify_admin_key(x_api_key: str = Header(None)):
    admin_key = os.getenv('ADMIN_API_KEY')
    if not x_api_key or x_api_key != admin_key:
        raise HTTPException(status_code=401, detail="Invalid API key")
    return True

def prepare_product_data(product: Product, images: List[ProductImage]) -> Dict[str, Any]:
    """DB ì œí’ˆ ë°ì´í„°ë¥¼ ì²­í‚¹ì— ì í•©í•œ í˜•íƒœë¡œ ì¤€ë¹„"""
    
    # ì œí’ˆ ê¸°ë³¸ ì •ë³´
    product_data = {
        'id': product.id,
        'name': product.product_name,
        'url': product.source_global_url or product.source_kr_url,
        'price': str(product.price) if product.price else '',
        'brand': 'UNCOMMON',
        'category': 'eyewear'
    }
    
    # ëª¨ë“  ì œí’ˆ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
    description_parts = []
    
    # color ì •ë³´
    if product.color:
        description_parts.append(f"ìƒ‰ìƒ: {product.color}")
    
    # description ì •ë³´ (JSON ì²˜ë¦¬)
    if product.description:
        desc_str = str(product.description)
        if desc_str and desc_str != '{}':
            description_parts.append(f"ì„¤ëª…: {desc_str}")
    
    # material ì •ë³´ (JSON ì²˜ë¦¬)  
    if product.material:
        material_str = str(product.material)
        if material_str and material_str != '{}':
            description_parts.append(f"ì¬ì§ˆ: {material_str}")
    
    # size ì •ë³´ (JSON ì²˜ë¦¬)
    if product.size:
        size_str = str(product.size)
        if size_str and size_str != '{}':
            description_parts.append(f"ì‚¬ì´ì¦ˆ: {size_str}")
    
    # reward_points ì •ë³´
    if product.reward_points:
        points_str = str(product.reward_points)
        if points_str and points_str != '{}':
            description_parts.append(f"ë¦¬ì›Œë“œ í¬ì¸íŠ¸: {points_str}")
    
    # ì„¤ëª… í†µí•©
    product_data['description'] = " | ".join(description_parts) if description_parts else ""
    
    # ì´ë¯¸ì§€ ì •ë³´
    if images:
        product_data['images'] = []
        for idx, img in enumerate(images):
            image_info = {
                'image_id': img.id,
                'image_order': img.image_order or idx,
                'size_bytes': len(img.image_data) if img.image_data else 0,
                'alt_text': f"ì œí’ˆ ì´ë¯¸ì§€ {idx + 1}",
                'context': f"ì œí’ˆ {product.product_name}ì˜ {idx + 1}ë²ˆì§¸ ì´ë¯¸ì§€"
            }
            product_data['images'].append(image_info)
    
    return product_data

async def process_products_indexing(product_ids: List[int] = None, force_reindex: bool = False):
    """ì œí’ˆ ì¸ë±ì‹± ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…"""
    db = next(get_db())
    errors = []
    indexed_count = 0
    
    try:
        # ì²˜ë¦¬í•  ì œí’ˆ ì„ íƒ
        query = db.query(Product)
        
        if product_ids:
            query = query.filter(Product.id.in_(product_ids))
        elif not force_reindex:
            query = query.filter(Product.indexed == False)
            
        products = query.all()
        
        logger.info(f"ğŸš€ {len(products)}ê°œ ì œí’ˆ ì¸ë±ì‹± ì‹œì‘")
        
        # ì œí’ˆë³„ ì²˜ë¦¬
        for product in products:
            try:
                logger.info(f"ğŸ“¦ ì œí’ˆ {product.id} ({product.product_name}) ì²˜ë¦¬ ì¤‘...")
                
                # ì´ë¯¸ì§€ ì •ë³´ ì¡°íšŒ
                images = db.query(ProductImage).filter(ProductImage.product_id == product.id).all()
                
                # ì œí’ˆ ë°ì´í„° ì¤€ë¹„
                product_data = prepare_product_data(product, images)
                
                # ì²­í‚¹
                chunks = chunker.chunk_product_data(product_data)
                logger.info(f"  ğŸ“„ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
                
                # ì²­í‚¹ ê²°ê³¼ ìƒì„¸ ì¶œë ¥
                for i, chunk in enumerate(chunks, 1):
                    logger.info(f"  ğŸ”µ ì²­í¬ {i}/{len(chunks)}:")
                    logger.info(f"     ğŸ“ ë‚´ìš©: {chunk.page_content[:200]}...")
                    logger.info(f"     ğŸ·ï¸  ë©”íƒ€ë°ì´í„°: {chunk.metadata}")
                
                if not chunks:
                    logger.warning(f"  âš ï¸ ì œí’ˆ {product.id}: ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                    continue
                
                # LangChain Document ë³€í™˜
                documents = []
                for chunk in chunks:
                    doc = Document(
                        page_content=chunk.page_content,
                        metadata=chunk.metadata
                    )
                    documents.append(doc)
                
                # Milvusì— ì €ì¥
                if documents:
                    vector_store.add_documents(documents)
                    logger.info(f"  âœ… ì œí’ˆ {product.id}: {len(documents)}ê°œ ë¬¸ì„œ Milvus ì €ì¥ ì™„ë£Œ")
                    
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    product.indexed = True
                    product.indexed_at = datetime.utcnow()
                    db.commit()
                    
                    indexed_count += 1
                else:
                    logger.warning(f"  âš ï¸ ì œí’ˆ {product.id}: ë¬¸ì„œê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                
            except Exception as e:
                error_msg = f"ì œí’ˆ {product.id} ì¸ë±ì‹± ì‹¤íŒ¨: {str(e)}"
                logger.error(error_msg)
                errors.append(error_msg)
                continue
        
        logger.info(f"ğŸ‰ ì¸ë±ì‹± ì™„ë£Œ: {indexed_count}ê°œ ì„±ê³µ, {len(errors)}ê°œ ì˜¤ë¥˜")
        return indexed_count, errors
        
    except Exception as e:
        error_msg = f"ì¸ë±ì‹± ì‘ì—… ì „ì²´ ì‹¤íŒ¨: {str(e)}"
        logger.error(error_msg)
        errors.append(error_msg)
        return indexed_count, errors
    finally:
        db.close()

# API ì—”ë“œí¬ì¸íŠ¸
@app.on_event("startup")
async def startup():
    """ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global embedding_model, vector_store, chunker
    
    logger.info("ğŸš€ UNCOMMON ì¸ë±ì‹± ì„œë¹„ìŠ¤ ì‹œì‘")
    
    try:
        # DB ì´ˆê¸°í™”
        init_db()
        logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        logger.info("ğŸ“¥ BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        embedding_model = get_bge_m3_model()
        logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # Milvus ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        logger.info("ğŸ”— Milvus ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘...")
        vector_store = ProductMilvusVectorStore(
            collection_name="uncommon_products",
            embedding_model=embedding_model,
            always_new=False  # ê¸°ì¡´ ë°ì´í„° ìœ ì§€
        )
        logger.info("âœ… Milvus ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ì²­í‚¹ ëª¨ë“ˆ ì´ˆê¸°í™”
        chunker = ProductTextChunker(chunk_size=500)
        logger.info("âœ… ì œí’ˆ ì²­í‚¹ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        
        logger.info("ğŸ‰ ëª¨ë“  ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

@app.get("/")
async def root():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "service": "UNCOMMON Indexing Service",
        "status": "running",
        "version": "1.0.0",
        "embedding_model": "BGE-M3",
        "vector_store": "Milvus"
    }

@app.post("/index/products", response_model=IndexResponse)
async def index_products(
    request: IndexRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    authorized: bool = Depends(verify_admin_key)
):
    """ì œí’ˆ ì¸ë±ì‹± ì‹œì‘"""
    
    # ì²˜ë¦¬í•  ì œí’ˆ ìˆ˜ í™•ì¸
    query = db.query(Product)
    
    if request.product_ids:
        query = query.filter(Product.id.in_(request.product_ids))
        total_count = len(request.product_ids)
    elif request.force_reindex:
        total_count = query.count()
    else:
        total_count = query.filter(Product.indexed == False).count()
    
    if total_count == 0:
        return IndexResponse(
            message="ì¸ë±ì‹±í•  ì œí’ˆì´ ì—†ìŠµë‹ˆë‹¤",
            total_products=0,
            indexed_count=0
        )
    
    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
    background_tasks.add_task(
        process_products_indexing,
        request.product_ids if request.product_ids else None,
        request.force_reindex
    )
    
    return IndexResponse(
        message=f"ì¸ë±ì‹± ì‹œì‘: {total_count}ê°œ ì œí’ˆ ì²˜ë¦¬ ì˜ˆì •",
        total_products=total_count,
        indexed_count=0
    )

@app.get("/index/stats", response_model=StatsResponse)
async def get_indexing_stats(
    db: Session = Depends(get_db),
    authorized: bool = Depends(verify_admin_key)
):
    """ì¸ë±ì‹± í†µê³„ ì¡°íšŒ"""
    
    total_products = db.query(Product).count()
    indexed_products = db.query(Product).filter(Product.indexed == True).count()
    pending_products = total_products - indexed_products
    
    # Milvus ë¬¸ì„œ ìˆ˜ í™•ì¸
    try:
        if vector_store and vector_store.collection:
            milvus_docs = vector_store.collection.num_entities
        else:
            milvus_docs = 0
    except:
        milvus_docs = 0
    
    return StatsResponse(
        total_products=total_products,
        indexed_products=indexed_products,
        pending_products=pending_products,
        milvus_documents=milvus_docs
    )

@app.post("/index/products/{product_id}")
async def index_single_product(
    product_id: int,
    db: Session = Depends(get_db),
    authorized: bool = Depends(verify_admin_key)
):
    """ë‹¨ì¼ ì œí’ˆ ì¦‰ì‹œ ì¸ë±ì‹±"""
    
    product = db.query(Product).filter(Product.id == product_id).first()
    if not product:
        raise HTTPException(status_code=404, detail="Product not found")
    
    try:
        # ì´ë¯¸ì§€ ì •ë³´ ì¡°íšŒ
        images = db.query(ProductImage).filter(ProductImage.product_id == product_id).all()
        
        # ì œí’ˆ ë°ì´í„° ì¤€ë¹„
        product_data = prepare_product_data(product, images)
        
        # ì²­í‚¹
        chunks = chunker.chunk_product_data(product_data)
        
        if not chunks:
            raise HTTPException(status_code=400, detail="No chunks generated for this product")
        
        # LangChain Document ë³€í™˜
        documents = []
        for chunk in chunks:
            doc = Document(
                page_content=chunk.page_content,
                metadata=chunk.metadata
            )
            documents.append(doc)
        
        # Milvusì— ì €ì¥
        vector_store.add_documents(documents)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        product.indexed = True
        product.indexed_at = datetime.utcnow()
        db.commit()
        
        return {
            "message": f"ì œí’ˆ {product_id} ì¸ë±ì‹± ì™„ë£Œ",
            "product_name": product.product_name,
            "chunks_created": len(chunks),
            "documents_indexed": len(documents)
        }
        
    except Exception as e:
        logger.error(f"ì œí’ˆ {product_id} ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/index/products/{product_id}")
async def remove_product_from_index(
    product_id: int,
    db: Session = Depends(get_db),
    authorized: bool = Depends(verify_admin_key)
):
    """ì œí’ˆì„ ì¸ë±ìŠ¤ì—ì„œ ì œê±° (ë¯¸êµ¬í˜„ - MVPì—ì„œëŠ” ì œì™¸)"""
    return {"message": "ê¸°ëŠ¥ ë¯¸êµ¬í˜„ - MVP ë‹¨ê³„ì—ì„œëŠ” ì œì™¸"}

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv('INDEXING_PORT', 8002))
    uvicorn.run(app, host="0.0.0.0", port=port)