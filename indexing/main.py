# RAG LLM ì‹œìŠ¤í…œì˜ ë²¡í„° ì¸ë±ì‹± ì„œë¹„ìŠ¤ ë©”ì¸ íŒŒì¼
# ëª©ì : ìŠ¤í¬ë˜í•‘ëœ ì œí’ˆ ë°ì´í„°ë¥¼ BGE-M3 ëª¨ë¸ë¡œ ë²¡í„°í™”í•˜ì—¬ Milvus DBì— ì €ì¥
# ê´€ë ¨ í•¨ìˆ˜: process_products_indexing (ë²¡í„°í™”), ProductTextChunker.chunk_product_data (ì²­í‚¹)
# ì£¼ìš” ê¸°ëŠ¥: í…ìŠ¤íŠ¸ ì²­í‚¹, BGE-M3 ì„ë² ë”©, Milvus ë²¡í„° ì €ì¥, ìë™ ì¸ë±ì‹±
"""
UNCOMMON ì œí’ˆ ì¸ë±ì‹± ì„œë¹„ìŠ¤ - BGE-M3 ì„ë² ë”© ëª¨ë¸ê³¼ Milvus ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™œìš©í•œ ì œí’ˆ ë°ì´í„° ë²¡í„°í™”
ëª©ì : PostgreSQLì˜ ì œí’ˆ ë°ì´í„°ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™”
"""

import os
import json
import logging  # ì¸ë±ì‹± ì‘ì—… ìƒì„¸ ë¡œê¹…
from datetime import datetime
from typing import List, Dict, Any
from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends
from pydantic import BaseModel  # API ìš”ì²­/ì‘ë‹µ ëª¨ë¸ ì •ì˜
from sqlalchemy.orm import Session  # PostgreSQL ORM ì„¸ì…˜
from langchain_core.documents import Document  # LangChain ë¬¸ì„œ í˜•íƒœë¡œ ë³€í™˜
from dotenv import load_dotenv  # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ

# í”„ë¡œì íŠ¸ í•µì‹¬ ëª¨ë“ˆ ì„í¬íŠ¸ - ê°ê° íŠ¹í™”ëœ ë²¡í„°í™” ê¸°ëŠ¥ ë‹´ë‹¹
from database import get_db, init_db, Product, ProductImage  # DB ì—°ê²° ë° ì œí’ˆ ëª¨ë¸
from text_chunker import ProductTextChunker, ProductChunk  # ì œí’ˆ íŠ¹í™” í…ìŠ¤íŠ¸ ì²­í‚¹
from embedding_generator import get_bge_m3_model  # BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë”
from milvus_client import ProductMilvusVectorStore  # Milvus ë²¡í„° ì €ì¥ì†Œ

# í™˜ê²½ë³€ìˆ˜ ë° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì„¤ì •
# .env.globalì—ì„œ Milvus, BGE-M3 ëª¨ë¸ ê´€ë ¨ ì„¤ì • ë¡œë“œ
load_dotenv('../.env.global')  # í”„ë¡œì íŠ¸ ì „ì—­ í™˜ê²½ë³€ìˆ˜
load_dotenv()  # ë¡œì»¬ í™˜ê²½ë³€ìˆ˜ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)

# ì¸ë±ì‹± ì‘ì—… ìƒì„¸ ë¡œê¹… ì„¤ì • - ë²¡í„°í™” ê³¼ì • ì¶”ì ìš©
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™” - ë²¡í„° ì¸ë±ì‹± ì „ìš© API ì„œë¹„ìŠ¤
app = FastAPI(
    title="UNCOMMON Indexing Service",
    description="ì œí’ˆ ë°ì´í„° ë²¡í„°í™” ë° Milvus ì¸ë±ì‹± ì„œë¹„ìŠ¤",
    version="1.0.0"
)

# ì „ì—­ AI/ML ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ - ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
embedding_model = None  # BGE-M3 ì„ë² ë”© ëª¨ë¸ (BAAI/bge-m3)
vector_store = None  # Milvus ë²¡í„° ìŠ¤í† ì–´ í´ë¼ì´ì–¸íŠ¸
chunker = None  # ì œí’ˆ í…ìŠ¤íŠ¸ ì²­í‚¹ ëª¨ë“ˆ

# API ìš”ì²­/ì‘ë‹µ ë°ì´í„° ëª¨ë¸ - í´ë¼ì´ì–¸íŠ¸ì™€ ì„œë²„ ê°„ ì¸ë±ì‹± ì‘ì—… íŒŒë¼ë¯¸í„° ì •ì˜
# IndexRequest: ì¸ë±ì‹± ì˜µì…˜ ì„¤ì • (ì „ì²´/ë¶€ë¶„, ê°•ì œ ì¬ì¸ë±ì‹±)
# IndexResponse: ì¸ë±ì‹± ê²°ê³¼ ì •ë³´ (ì„±ê³µ/ì‹¤íŒ¨ ê±´ìˆ˜, ì˜¤ë¥˜ ë©”ì‹œì§€)
# StatsResponse: ì‹œìŠ¤í…œ ì „ì²´ ì¸ë±ì‹± í˜„í™© í†µê³„
class IndexRequest(BaseModel):
    force_reindex: bool = False  # ê¸°ì¡´ ì¸ë±ì‹±ëœ ì œí’ˆë„ ì¬ì²˜ë¦¬ ì—¬ë¶€
    product_ids: List[int] = []  # íŠ¹ì • ì œí’ˆ IDë§Œ ì¸ë±ì‹± (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ ì „ì²´)

class IndexResponse(BaseModel):
    message: str  # ì¸ë±ì‹± ì‘ì—… ìƒíƒœ ë©”ì‹œì§€
    total_products: int  # ì²˜ë¦¬ ëŒ€ìƒ ì œí’ˆ ì´ ê°œìˆ˜
    indexed_count: int  # ì„±ê³µì ìœ¼ë¡œ ì¸ë±ì‹±ëœ ì œí’ˆ ìˆ˜
    errors: List[str] = []  # ì¸ë±ì‹± ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ ëª©ë¡

class StatsResponse(BaseModel):
    total_products: int  # PostgreSQL ì „ì²´ ì œí’ˆ ìˆ˜
    indexed_products: int  # ì¸ë±ì‹± ì™„ë£Œëœ ì œí’ˆ ìˆ˜
    pending_products: int  # ì¸ë±ì‹± ëŒ€ê¸° ì¤‘ì¸ ì œí’ˆ ìˆ˜
    milvus_documents: int  # Milvusì— ì €ì¥ëœ ì‹¤ì œ ë²¡í„° ë¬¸ì„œ ìˆ˜

# Admin authentication removed for MVP

# ì œí’ˆ ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ - PostgreSQL ì œí’ˆ ë°ì´í„°ë¥¼ ë²¡í„°í™”ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë³€í™˜
# ëª©ì : DBì˜ ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ë¡œ í†µí•©, ë‹¤êµ­ì–´ ì •ë³´ ë³‘í•©
# ê´€ë ¨ í•¨ìˆ˜: ProductTextChunker.chunk_product_data (ì²­í‚¹ ì²˜ë¦¬)
# ì…ë ¥: Product ëª¨ë¸, ProductImage ë¦¬ìŠ¤íŠ¸
# ì¶œë ¥: ì²­í‚¹ì— ì í•©í•œ Dict í˜•íƒœ ì œí’ˆ ì •ë³´
def prepare_product_data(product: Product, images: List[ProductImage]) -> Dict[str, Any]:
    """DB ì œí’ˆ ë°ì´í„°ë¥¼ ì²­í‚¹ì— ì í•©í•œ í˜•íƒœë¡œ ì¤€ë¹„ - JSONB í•„ë“œ íŒŒì‹± ë° í…ìŠ¤íŠ¸ í†µí•©"""
    
    # ì œí’ˆ ë©”íƒ€ë°ì´í„° êµ¬ì„± - ê²€ìƒ‰ ì‹œ í•„í„°ë§ ë° ê²°ê³¼ í‘œì‹œìš©
    product_data = {
        'id': product.id,  # ì œí’ˆ ê³ ìœ  ì‹ë³„ì
        'name': product.product_name,  # ì œí’ˆëª… (ì£¼ ê²€ìƒ‰ ëŒ€ìƒ)
        'url': product.source_global_url or product.source_kr_url,  # ì œí’ˆ í˜ì´ì§€ ë§í¬
        'price': str(product.price) if product.price else '',  # ê°€ê²© ì •ë³´
        'brand': 'UNCOMMON',  # ë¸Œëœë“œëª… (ê³ ì •ê°’)
        'category': 'eyewear'  # ì œí’ˆ ì¹´í…Œê³ ë¦¬ (ì•ˆê²½)
    }
    
    # ì œí’ˆì˜ ëª¨ë“  ì†ì„± ì •ë³´ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ í†µí•©
    # PostgreSQL JSONB í•„ë“œë“¤ì„ íŒŒì‹±í•˜ì—¬ ìì—°ì–´ í˜•íƒœë¡œ ë³€í™˜
    description_parts = []
    
    # ìƒ‰ìƒ ì •ë³´ ì¶”ê°€ - ì‚¬ìš©ìê°€ "ë¹¨ê°„ ì•ˆê²½" ë“±ìœ¼ë¡œ ê²€ìƒ‰ ì‹œ ë§¤ì¹­
    if product.color:
        description_parts.append(f"ìƒ‰ìƒ: {product.color}")
    
    # ì œí’ˆ ìƒì„¸ ì„¤ëª… (JSONB) - ì˜ë¬¸/í•œê¸€ ë²„ì „ ëª¨ë‘ í¬í•¨
    if product.description:
        desc_str = str(product.description)
        if desc_str and desc_str != '{}':
            description_parts.append(f"ì„¤ëª…: {desc_str}")
    
    # ì¬ì§ˆ/ì†Œì¬ ì •ë³´ (JSONB) - "ì•„ì„¸í…Œì´íŠ¸ ì•ˆê²½" ë“± ì¬ì§ˆ ê¸°ë°˜ ê²€ìƒ‰ ì§€ì›
    if product.material:
        material_str = str(product.material)
        if material_str and material_str != '{}':
            description_parts.append(f"ì¬ì§ˆ: {material_str}")
    
    # ì‚¬ì´ì¦ˆ ì •ë³´ (JSONB) - "í° ì•ˆê²½", "ì‘ì€ í”„ë ˆì„" ë“± í¬ê¸° ê´€ë ¨ ê²€ìƒ‰
    if product.size:
        size_str = str(product.size)
        if size_str and size_str != '{}':
            description_parts.append(f"ì‚¬ì´ì¦ˆ: {size_str}")
    
    # ë¦¬ì›Œë“œ í¬ì¸íŠ¸ ì •ë³´ - í˜œíƒ ê´€ë ¨ ê²€ìƒ‰ ì‹œ í™œìš©
    if product.reward_points:
        points_str = str(product.reward_points)
        if points_str and points_str != '{}':
            description_parts.append(f"ë¦¬ì›Œë“œ í¬ì¸íŠ¸: {points_str}")
    
    # ëª¨ë“  ì œí’ˆ ì†ì„±ì„ í•˜ë‚˜ì˜ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ í†µí•©
    product_data['description'] = " | ".join(description_parts) if description_parts else ""
    
    # ì œí’ˆ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° êµ¬ì„± - ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì§€ì›ìš©
    # ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ëŠ” PostgreSQLì— ì €ì¥, ë©”íƒ€ë°ì´í„°ë§Œ ë²¡í„°í™”
    if images:
        product_data['images'] = []
        for idx, img in enumerate(images):
            # ê° ì´ë¯¸ì§€ì˜ ê²€ìƒ‰ ê°€ëŠ¥í•œ ë©”íƒ€ë°ì´í„° ìƒì„±
            image_info = {
                'image_id': img.id,  # ì´ë¯¸ì§€ DB ê³ ìœ  ID
                'image_order': img.image_order or idx,  # ì´ë¯¸ì§€ í‘œì‹œ ìˆœì„œ
                'size_bytes': len(img.image_data) if img.image_data else 0,  # ì´ë¯¸ì§€ í¬ê¸°
                'alt_text': f"ì œí’ˆ ì´ë¯¸ì§€ {idx + 1}",  # ëŒ€ì²´ í…ìŠ¤íŠ¸
                'context': f"ì œí’ˆ {product.product_name}ì˜ {idx + 1}ë²ˆì§¸ ì´ë¯¸ì§€"  # ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸
            }
            product_data['images'].append(image_info)
    
    return product_data

async def process_products_indexing(product_ids: List[int] = None, force_reindex: bool = False):
    """ì œí’ˆ ì¸ë±ì‹± ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…"""
    db = next(get_db())
    errors = []
    indexed_count = 0
    
    try:
        # ì²˜ë¦¬í•  ì œí’ˆ ì„ íƒ
        query = db.query(Product)
        
        if product_ids:
            query = query.filter(Product.id.in_(product_ids))
        elif not force_reindex:
            query = query.filter(Product.indexed == False)
            
        products = query.all()
        
        logger.info(f"ğŸš€ {len(products)}ê°œ ì œí’ˆ ì¸ë±ì‹± ì‹œì‘")
        
        # ì œí’ˆë³„ ì²˜ë¦¬
        for product in products:
            try:
                logger.info(f"ğŸ“¦ ì œí’ˆ {product.id} ({product.product_name}) ì²˜ë¦¬ ì¤‘...")
                
                # ì´ë¯¸ì§€ ì •ë³´ ì¡°íšŒ
                images = db.query(ProductImage).filter(ProductImage.product_id == product.id).all()
                
                # ì œí’ˆ ë°ì´í„° ì¤€ë¹„
                product_data = prepare_product_data(product, images)
                
                # ì²­í‚¹
                chunks = chunker.chunk_product_data(product_data)
                logger.info(f"  ğŸ“„ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
                
                # ì²­í‚¹ ê²°ê³¼ ìƒì„¸ ì¶œë ¥
                for i, chunk in enumerate(chunks, 1):
                    logger.info(f"  ğŸ”µ ì²­í¬ {i}/{len(chunks)}:")
                    logger.info(f"     ğŸ“ ë‚´ìš©: {chunk.page_content[:200]}...")
                    logger.info(f"     ğŸ·ï¸  ë©”íƒ€ë°ì´í„°: {chunk.metadata}")
                
                if not chunks:
                    logger.warning(f"  âš ï¸ ì œí’ˆ {product.id}: ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                    continue
                
                # LangChain Document ë³€í™˜
                documents = []
                for chunk in chunks:
                    doc = Document(
                        page_content=chunk.page_content,
                        metadata=chunk.metadata
                    )
                    documents.append(doc)
                
                # Milvusì— ì €ì¥
                if documents:
                    vector_store.add_documents(documents)
                    logger.info(f"  âœ… ì œí’ˆ {product.id}: {len(documents)}ê°œ ë¬¸ì„œ Milvus ì €ì¥ ì™„ë£Œ")
                    
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    product.indexed = True
                    product.indexed_at = datetime.utcnow()
                    db.commit()
                    
                    indexed_count += 1
                else:
                    logger.warning(f"  âš ï¸ ì œí’ˆ {product.id}: ë¬¸ì„œê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                
            except Exception as e:
                error_msg = f"ì œí’ˆ {product.id} ì¸ë±ì‹± ì‹¤íŒ¨: {str(e)}"
                logger.error(error_msg)
                errors.append(error_msg)
                continue
        
        logger.info(f"ğŸ‰ ì¸ë±ì‹± ì™„ë£Œ: {indexed_count}ê°œ ì„±ê³µ, {len(errors)}ê°œ ì˜¤ë¥˜")
        return indexed_count, errors
        
    except Exception as e:
        error_msg = f"ì¸ë±ì‹± ì‘ì—… ì „ì²´ ì‹¤íŒ¨: {str(e)}"
        logger.error(error_msg)
        errors.append(error_msg)
        return indexed_count, errors
    finally:
        db.close()

# API ì—”ë“œí¬ì¸íŠ¸
@app.on_event("startup")
async def startup():
    """ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global embedding_model, vector_store, chunker
    
    logger.info("ğŸš€ UNCOMMON ì¸ë±ì‹± ì„œë¹„ìŠ¤ ì‹œì‘")
    
    try:
        # DB ì´ˆê¸°í™”
        init_db()
        logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        logger.info("ğŸ“¥ BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        embedding_model = get_bge_m3_model()
        logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # Milvus ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        logger.info("ğŸ”— Milvus ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘...")
        vector_store = ProductMilvusVectorStore(
            collection_name="uncommon_products",
            embedding_model=embedding_model,
            always_new=False  # ê¸°ì¡´ ë°ì´í„° ìœ ì§€
        )
        logger.info("âœ… Milvus ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ì²­í‚¹ ëª¨ë“ˆ ì´ˆê¸°í™”
        chunker = ProductTextChunker(chunk_size=500)
        logger.info("âœ… ì œí’ˆ ì²­í‚¹ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        
        logger.info("ğŸ‰ ëª¨ë“  ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

@app.get("/")
async def root():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "service": "UNCOMMON Indexing Service",
        "status": "running",
        "version": "1.0.0",
        "embedding_model": "BGE-M3",
        "vector_store": "Milvus"
    }

@app.get("/health")
async def health_check():
    """ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬"""
    return {
        "status": "healthy",
        "service": "indexing"
    }

@app.post("/index/products", response_model=IndexResponse)
async def index_products(
    request: IndexRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """ì œí’ˆ ì¸ë±ì‹± ì‹œì‘"""
    
    # ì²˜ë¦¬í•  ì œí’ˆ ìˆ˜ í™•ì¸
    query = db.query(Product)
    
    if request.product_ids:
        query = query.filter(Product.id.in_(request.product_ids))
        total_count = len(request.product_ids)
    elif request.force_reindex:
        total_count = query.count()
    else:
        total_count = query.filter(Product.indexed == False).count()
    
    if total_count == 0:
        return IndexResponse(
            message="ì¸ë±ì‹±í•  ì œí’ˆì´ ì—†ìŠµë‹ˆë‹¤",
            total_products=0,
            indexed_count=0
        )
    
    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
    background_tasks.add_task(
        process_products_indexing,
        request.product_ids if request.product_ids else None,
        request.force_reindex
    )
    
    return IndexResponse(
        message=f"ì¸ë±ì‹± ì‹œì‘: {total_count}ê°œ ì œí’ˆ ì²˜ë¦¬ ì˜ˆì •",
        total_products=total_count,
        indexed_count=0
    )

@app.get("/index/stats", response_model=StatsResponse)
async def get_indexing_stats(
    db: Session = Depends(get_db)
):
    """ì¸ë±ì‹± í†µê³„ ì¡°íšŒ"""
    
    total_products = db.query(Product).count()
    indexed_products = db.query(Product).filter(Product.indexed == True).count()
    pending_products = total_products - indexed_products
    
    # Milvus ë¬¸ì„œ ìˆ˜ í™•ì¸
    try:
        if vector_store and vector_store.collection:
            milvus_docs = vector_store.collection.num_entities
        else:
            milvus_docs = 0
    except:
        milvus_docs = 0
    
    return StatsResponse(
        total_products=total_products,
        indexed_products=indexed_products,
        pending_products=pending_products,
        milvus_documents=milvus_docs
    )

@app.post("/index/products/{product_id}")
async def index_single_product(
    product_id: int,
    db: Session = Depends(get_db)
):
    """ë‹¨ì¼ ì œí’ˆ ì¦‰ì‹œ ì¸ë±ì‹±"""
    
    product = db.query(Product).filter(Product.id == product_id).first()
    if not product:
        raise HTTPException(status_code=404, detail="Product not found")
    
    try:
        # ì´ë¯¸ì§€ ì •ë³´ ì¡°íšŒ
        images = db.query(ProductImage).filter(ProductImage.product_id == product_id).all()
        
        # ì œí’ˆ ë°ì´í„° ì¤€ë¹„
        product_data = prepare_product_data(product, images)
        
        # ì²­í‚¹
        chunks = chunker.chunk_product_data(product_data)
        
        if not chunks:
            raise HTTPException(status_code=400, detail="No chunks generated for this product")
        
        # LangChain Document ë³€í™˜
        documents = []
        for chunk in chunks:
            doc = Document(
                page_content=chunk.page_content,
                metadata=chunk.metadata
            )
            documents.append(doc)
        
        # Milvusì— ì €ì¥
        vector_store.add_documents(documents)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        product.indexed = True
        product.indexed_at = datetime.utcnow()
        db.commit()
        
        return {
            "message": f"ì œí’ˆ {product_id} ì¸ë±ì‹± ì™„ë£Œ",
            "product_name": product.product_name,
            "chunks_created": len(chunks),
            "documents_indexed": len(documents)
        }
        
    except Exception as e:
        logger.error(f"ì œí’ˆ {product_id} ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/index/products/{product_id}")
async def remove_product_from_index(
    product_id: int,
    db: Session = Depends(get_db)
):
    """ì œí’ˆì„ ì¸ë±ìŠ¤ì—ì„œ ì œê±° (ë¯¸êµ¬í˜„ - MVPì—ì„œëŠ” ì œì™¸)"""
    return {"message": "ê¸°ëŠ¥ ë¯¸êµ¬í˜„ - MVP ë‹¨ê³„ì—ì„œëŠ” ì œì™¸"}

@app.post("/process/new-products")
async def process_new_products(
    request: dict,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """ìŠ¤í¬ë˜í•‘ ì™„ë£Œ í›„ ìë™ ì¸ë±ì‹± ì‹œì‘ (ìŠ¤í¬ë˜í¼ì—ì„œ í˜¸ì¶œ)"""
    
    products_count = request.get("products_count", 0)
    logger.info(f"ğŸ“¬ ìŠ¤í¬ë˜í¼ë¡œë¶€í„° ì•Œë¦¼ ë°›ìŒ: {products_count}ê°œ ì œí’ˆ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
    
    # ì¸ë±ì‹±ë˜ì§€ ì•Šì€ ì œí’ˆ ìˆ˜ í™•ì¸
    pending_count = db.query(Product).filter(Product.indexed == False).count()
    
    if pending_count == 0:
        return {
            "message": f"ìŠ¤í¬ë˜í•‘ ì•Œë¦¼ ìˆ˜ì‹ : {products_count}ê°œ ì œí’ˆ, ì¸ë±ì‹±í•  ì œí’ˆ ì—†ìŒ",
            "products_scraped": products_count,
            "products_to_index": 0
        }
    
    # ë°±ê·¸ë¼ìš´ë“œ ìë™ ì¸ë±ì‹± ì‹œì‘
    background_tasks.add_task(
        process_products_indexing,
        None,  # product_ids = None (ëª¨ë“  ë¯¸ì¸ë±ì‹± ì œí’ˆ)
        False  # force_reindex = False
    )
    
    logger.info(f"ğŸš€ ìë™ ì¸ë±ì‹± ì‹œì‘: {pending_count}ê°œ ì œí’ˆ ì²˜ë¦¬ ì˜ˆì •")
    
    return {
        "message": f"ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ì•Œë¦¼ ìˆ˜ì‹ , ìë™ ì¸ë±ì‹± ì‹œì‘",
        "products_scraped": products_count,
        "products_to_index": pending_count,
        "status": "indexing_started"
    }

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ['INDEXING_INTERNAL_PORT'])
    uvicorn.run(app, host="0.0.0.0", port=port)