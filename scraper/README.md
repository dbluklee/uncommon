# Scraper ì„œë¹„ìŠ¤

## ğŸ“‹ ê°œìš”
UNCOMMON ì•„ì´ì›¨ì–´ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì œí’ˆ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ì›¹ ìŠ¤í¬ë˜í•‘ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. FastAPI ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìœ¼ë©°, ì œí’ˆ ë°ì´í„°ì™€ ì´ë¯¸ì§€ë¥¼ PostgreSQLì— ì €ì¥í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥
- **ì›¹ ìŠ¤í¬ë˜í•‘**: UNCOMMON ì›¹ì‚¬ì´íŠ¸ ì œí’ˆ ì •ë³´ ìë™ ìˆ˜ì§‘
- **ì´ë¯¸ì§€ ì²˜ë¦¬**: ì œí’ˆ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ë°”ì´ë„ˆë¦¬ ì €ì¥
- **ë°ì´í„° ê²€ì¦**: ì¤‘ë³µ ì œí’ˆ ì²´í¬ ë° ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥
- **ì‘ì—… ê´€ë¦¬**: ìŠ¤í¬ë˜í•‘ ì‘ì—… ìƒíƒœ ì¶”ì  ë° ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### í™˜ê²½ë³€ìˆ˜
```bash
SCRAPER_PORT=8001                    # ì™¸ë¶€ ì ‘ê·¼ í¬íŠ¸
SCRAPER_INTERNAL_PORT=8000          # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ í¬íŠ¸
TARGET_URL=https://ucmeyewear.earth/category/all/87/
```

### ì‹¤í–‰ ëª…ë ¹
```bash
# ì„œë¹„ìŠ¤ ì‹œì‘
cd scraper
source ../.env.global
docker compose up -d

# ë¡œì»¬ ê°œë°œ ì‹¤í–‰
pip install -r requirements.txt
python main.py
```

### ì ‘ì† ì •ë³´
- **API ì„œë²„**: `http://localhost:8001`
- **Swagger UI**: `http://localhost:8001/docs`
- **ReDoc**: `http://localhost:8001/redoc`

## ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸

### 1. í—¬ìŠ¤ì²´í¬
```http
GET /health
```

**ì‘ë‹µ ì˜ˆì‹œ:**
```json
{
    "status": "healthy",
    "service": "scraper",
    "timestamp": "2024-01-10T10:30:00Z"
}
```

### 2. ìŠ¤í¬ë˜í•‘ ì‹œì‘
```http
POST /scrape
Content-Type: application/json

{
    "target_url": "https://ucmeyewear.earth/category/all/87/"
}
```

**ì‘ë‹µ ì˜ˆì‹œ:**
```json
{
    "job_id": 123,
    "message": "ìŠ¤í¬ë˜í•‘ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
    "target_url": "https://ucmeyewear.earth/category/all/87/",
    "status": "running"
}
```

### 3. ì‘ì—… ìƒíƒœ í™•ì¸
```http
GET /scrape/job/{job_id}
```

**ì‘ë‹µ ì˜ˆì‹œ:**
```json
{
    "id": 123,
    "target_url": "https://ucmeyewear.earth/category/all/87/",
    "status": "completed",
    "products_count": 25,
    "started_at": "2024-01-10T10:30:00Z",
    "completed_at": "2024-01-10T10:32:15Z"
}
```

## ğŸ“Š ì…ë ¥/ì¶œë ¥ ë°ì´í„° í˜•ì‹

### ì…ë ¥ ë°ì´í„°
```json
{
    "target_url": "https://ucmeyewear.earth/category/all/87/"
}
```

### ì¶œë ¥ ë°ì´í„° (PostgreSQL ì €ì¥)
```python
# Product ë°ì´í„° êµ¬ì¡°
{
    "id": 123,
    "source_global_url": "https://ucmeyewear.earth/products/frame-001",
    "source_kr_url": "https://ucmeyewear.earth/ko/products/frame-001",
    "product_name": "UNCOMMON Titanium Frame",
    "color": "Black",
    "price": {
        "global": "$199.00",
        "kr": "259,000ì›"
    },
    "reward_points": {
        "global": "199 points",
        "kr": "2,590 í¬ì¸íŠ¸"
    },
    "description": {
        "global": "Premium titanium eyewear frame with acetate temples",
        "kr": "ì•„ì„¸í…Œì´íŠ¸ í…œí”Œì´ ìˆëŠ” í”„ë¦¬ë¯¸ì—„ í‹°íƒ€ëŠ„ ì•„ì´ì›¨ì–´ í”„ë ˆì„"
    },
    "material": {
        "global": "Titanium, Acetate",
        "kr": "í‹°íƒ€ëŠ„, ì•„ì„¸í…Œì´íŠ¸"
    },
    "size": {
        "global": "Medium (52-18-145)",
        "kr": "ë¯¸ë””ì—„ (52-18-145)"
    },
    "issoldout": false,
    "indexed": false,
    "scraped_at": "2024-01-10T10:30:00Z"
}

# Product Image ë°ì´í„° êµ¬ì¡°  
{
    "id": 456,
    "product_id": 123,
    "image_data": b"\\x89PNG\\r\\n...",  # ë°”ì´ë„ˆë¦¬ ì´ë¯¸ì§€ ë°ì´í„°
    "image_order": 0
}
```

## ğŸ”„ í†µì‹  ë°©ì‹

### HTTP REST API
- **í”„ë¡œí† ì½œ**: HTTP/1.1
- **í¬ë§·**: JSON
- **ì¸ì½”ë”©**: UTF-8
- **ì¸ì¦**: ì—†ìŒ (MVP ë‹¨ê³„)

### ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™
- **ORM**: SQLAlchemy 2.0.23
- **ì—°ê²°**: ë¹„ë™ê¸° PostgreSQL ì—°ê²°
- **íŠ¸ëœì­ì…˜**: ìë™ ì»¤ë°‹/ë¡¤ë°±

### ì™¸ë¶€ ì›¹ì‚¬ì´íŠ¸ í†µì‹ 
```python
# HTTP ìš”ì²­ ì„¤ì •
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive'
}

# ìš”ì²­ ê°„ ë”œë ˆì´
sleep_time = random.uniform(1, 3)  # 1-3ì´ˆ ëœë¤ ë”œë ˆì´
```

## ğŸ”— ì˜ì¡´ì„±

### í•„ìˆ˜ ì˜ì¡´ì„±
```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
sqlalchemy==2.0.23
asyncpg==0.29.0
requests==2.31.0
beautifulsoup4==4.12.2
Pillow==10.1.0
python-multipart==0.0.6
```

### ì‹œìŠ¤í…œ ì˜ì¡´ì„±
- **Python 3.11+**
- **PostgreSQL ë°ì´í„°ë² ì´ìŠ¤**: ì œí’ˆ ë°ì´í„° ì €ì¥
- **Docker & Docker Compose**: ì»¨í…Œì´ë„ˆ ì‹¤í–‰ í™˜ê²½

### ì—°ê´€ ì„œë¹„ìŠ¤
- **PostgreSQL DB**: ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„° ì €ì¥
- **Indexing Service**: ìŠ¤í¬ë˜í•‘ ì™„ë£Œ í›„ ë²¡í„° ì¸ë±ì‹± íŠ¸ë¦¬ê±°

## ğŸ› ï¸ ìŠ¤í¬ë˜í•‘ ë¡œì§

### 1. ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘
```python
def scrape_product_list(category_url):
    # ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ì—ì„œ ì œí’ˆ URL ìˆ˜ì§‘
    response = requests.get(category_url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    product_links = []
    for link in soup.find_all('a', class_='product-item'):
        product_url = urljoin(category_url, link.get('href'))
        product_links.append(product_url)
    
    return product_links
```

### 2. ê°œë³„ ì œí’ˆ ì •ë³´ ìˆ˜ì§‘
```python
def scrape_product_detail(product_url):
    response = requests.get(product_url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    product_data = {
        'product_name': soup.find('h1', class_='product-title').text.strip(),
        'color': soup.find('span', class_='color-name').text.strip(),
        'price': extract_price_info(soup),
        'description': extract_description(soup),
        'material': extract_material_info(soup),
        'size': extract_size_info(soup),
        'images': extract_image_urls(soup)
    }
    
    return product_data
```

### 3. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
```python
def download_and_save_images(product_id, image_urls):
    for order, img_url in enumerate(image_urls):
        img_response = requests.get(img_url)
        if img_response.status_code == 200:
            # ì´ë¯¸ì§€ë¥¼ ë°”ì´ë„ˆë¦¬ë¡œ ì €ì¥
            image_data = img_response.content
            save_product_image(product_id, image_data, order)
```

## ğŸ“ˆ ì„±ëŠ¥ ë° ëª¨ë‹ˆí„°ë§

### ìŠ¤í¬ë˜í•‘ ì„±ëŠ¥
- **ì²˜ë¦¬ ì†ë„**: ì œí’ˆë‹¹ 2-3ì´ˆ
- **ë™ì‹œ ì²˜ë¦¬**: ìˆœì°¨ ì²˜ë¦¬ (ì‚¬ì´íŠ¸ ë¶€í•˜ ë°©ì§€)
- **ì¬ì‹œë„ ë¡œì§**: ì‹¤íŒ¨ ì‹œ ìµœëŒ€ 3íšŒ ì¬ì‹œë„

### ì—ëŸ¬ ì²˜ë¦¬
```python
try:
    product_data = scrape_product_detail(url)
    save_product(product_data)
except requests.RequestException as e:
    logger.error(f"Network error: {e}")
    retry_count += 1
except Exception as e:
    logger.error(f"Parsing error: {e}")
    skip_product()
```

### ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
```python
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# ì§„í–‰ìƒí™© ë¡œê¹…
logger.info(f"ìŠ¤í¬ë˜í•‘ ì‹œì‘: {target_url}")
logger.info(f"ì œí’ˆ ìˆ˜ì§‘ ì™„ë£Œ: {product_count}ê°œ")
logger.info(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ: {image_count}ê°œ")
```

## âš ï¸ ì£¼ì˜ì‚¬í•­
- **ì›¹ì‚¬ì´íŠ¸ ì •ì±…**: robots.txt ì¤€ìˆ˜ ë° ê³¼ë„í•œ ìš”ì²­ ë°©ì§€
- **ë²•ì  ê³ ë ¤ì‚¬í•­**: ì €ì‘ê¶Œ ë° ì´ìš©ì•½ê´€ ì¤€ìˆ˜ í•„ìš”
- **ë°ì´í„° í’ˆì§ˆ**: ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„° ê²€ì¦ ë° ì •ì œ ê³¼ì • í•„ìš”
- **ì‚¬ì´íŠ¸ ë³€ê²½**: ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ ì‹œ íŒŒì‹± ë¡œì§ ì—…ë°ì´íŠ¸ í•„ìš”
- **ì—ëŸ¬ ë³µêµ¬**: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë° íŒŒì‹± ì˜¤ë¥˜ì— ëŒ€í•œ ì ì ˆí•œ ì²˜ë¦¬